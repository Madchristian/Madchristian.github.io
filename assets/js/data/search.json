[ { "title": "TunnelDebugger Support", "url": "/posts/tunneldebugger/", "categories": "Apple, Apps, Swift", "tags": "iOS, VPN Debugging", "date": "2025-04-13 09:51:16 +0200", "snippet": "tunneldebugger – Der umfassende Support-LeitfadenWillkommen auf unserem Support-Blog für tunneldebugger – die App zur Netzwerk- und VPN-Diagnose! In diesem Beitrag erfährst du, wie meine App funkti...", "content": "tunneldebugger – Der umfassende Support-LeitfadenWillkommen auf unserem Support-Blog für tunneldebugger – die App zur Netzwerk- und VPN-Diagnose! In diesem Beitrag erfährst du, wie meine App funktioniert, welche Features dir zur Verfügung stehen und wie du typische Probleme lösen kannst. Ob du IT-Administrator oder Endnutzer bist – dieser Leitfaden bietet dir wertvolle Einblicke in die Funktionsweise und Bedienung von tunneldebugger.EinführungIn modernen Unternehmensnetzwerken werden VPN-Lösungen häufig eingesetzt – insbesondere im Rahmen von MDM-On-Demand-VPN-Konzepten. Um sicherzustellen, dass diese Verbindungen reibungslos funktionieren und eventuelle Verbindungsprobleme zeitnah behoben werden können, haben wir tunneldebugger entwickelt. Die App ermöglicht es, VPN-Verbindungen und Netzwerkpfade detailliert zu analysieren, indem sie – über verschiedene Netzwerktools – wichtige Kennzahlen wie Router-IP, Ping-Roundtrip-Time (RTT) oder HTTP-Erreichbarkeit ermittelt.FunktionsweiseInfoTabWLAN- und Geräteinformationen WLAN-Daten:Mithilfe der CaptiveNetwork-APIs ermittelt tunneldebugger die aktuelle WLAN-SSID und BSSID. Diese Informationen sind hilfreich, um festzustellen, mit welchem Netzwerk dein Gerät gerade verbunden ist – besonders wichtig in Umgebungen, in denen mehrere Netzwerke aktiv sein können. Geräteinformationen:Zusätzlich zeigt die App grundlegende Systeminformationen an, wie den Namen deines Geräts, den Systemnamen (z. B. iOS) und die aktuelle iOS-Version. Auch die lokale IP-Adresse wird angezeigt, sodass du sofort weißt, unter welcher IP dein Gerät im Netzwerk erreichbar ist.PortCheck Zielverwaltung:Jedes Ziel wird als Instanz von PortCheckTarget modelliert. Diese enthält: Hostname &amp; Port: Die grundlegenden Parameter, die angeben, wohin die Verbindung aufgebaut werden soll. Statusinformationen: isReachable gibt an, ob eine Verbindung erfolgreich hergestellt werden konnte; errorMessage speichert Fehlermeldungen; resolvedIP enthält die über DNS ermittelte IP-Adresse; rtt misst die Round-Trip-Time, also die Zeit, die benötigt wird, um eine Antwort zu erhalten. Weitere Angaben: Angaben wie lastChecked (Zeitpunkt der letzten Überprüfung) und isChecking (ob aktuell eine Prüfung läuft) helfen, den Status kontinuierlich zu aktualisieren. Asynchrone Überprüfung:Die PortCheckViewModel-Klasse koordiniert die Prüfung aller konfigurierten Ziele. Für jedes Ziel wird: Zunächst der Hostname in eine IP-Adresse aufgelöst (mithilfe einer asynchronen DNS-Auflösung). Anschließend wird versucht, über eine TCP-Verbindung (unter Nutzung des Network‑Frameworks) den Zielport zu erreichen. Der Verbindungsversuch startet eine Zeitmessung: Das gemessene Zeitintervall entspricht der RTT. Falls die Verbindung innerhalb einer bestimmten Zeit (z. B. 30 Sekunden) nicht hergestellt werden kann, wird das Ziel als nicht erreichbar markiert. Ergebnisdarstellung:Die Resultate der Überprüfung (Erreichbarkeit, RTT, Fehlerdetails) werden in der Benutzeroberfläche in einer Tabelle (Form) angezeigt. Du findest dort separate Abschnitte für die konfigurierten Portziele, inklusive aller Statusinformationen und Messwerte. Außerdem sorgt ein pull-to-refresh dafür, dass der Benutzer den Test manuell neu starten kann. TracerouteTabDie Traceroute in tunneldebugger bietet eine umfassende Ansicht, mit der Netzwerkadministratoren und technisch interessierte Anwender den Weg von ICMP-Paketen durch ihr Netzwerk visualisieren können. Der obere Bereich erlaubt eine schnelle Eingabe des gewünschten Ziels und das Starten bzw. Stoppen des Traceroute-Vorgangs. Die Ergebnisse werden Zeile für Zeile angezeigt, wobei jede Zeile interaktiv ist – so können Benutzer direkt auf einen Hop tippen, um weitere Aktionen (z. B. Kopieren der IP, Whois-Abfrage) durchzuführen. Der gesamte Prozess ist asynchron aufgebaut, sodass Änderungen in Echtzeit in der UI erscheinen.Dieser Aufbau unterstützt nicht nur die Diagnose von Netzwerkpfaden, sondern ermöglicht auch eine intuitive Interaktion mit den einzelnen Netzwerkknoten – was bei der Fehlersuche in VPN- oder lokalen Netzwerken äußerst hilfreich sein kann.Einstellungen TabDer Einstellungen-Tab in tunneldebugger bietet dir auf einen Blick alle wichtigen Berechtigungs- und Debug-Einstellungen – von der Standortfreigabe über den lokalen Netzwerkzugriff bis hin zur Log-Steuerung. Er ermöglicht es, systematisch zu überprüfen, ob alle Berechtigungen erteilt sind, und bietet die Möglichkeit, den Detailgrad der Logausgaben (also den Debug-Modus) zu steuern. Hier ein Überblick:Einstellungen Tab Lokaler Netzwerkzugriff:Es wird angezeigt, ob der Zugriff auf das lokale Netzwerk erlaubt wurde – dies geschieht typischerweise über einen Test (z. B. mit einem UDP-Test). Ist der Test erfolgreich, erscheint ein grünes Symbol (wie zum Beispiel ein „wifi“-Icon), andernfalls ein rotes Symbol. Falls der Zugriff auf das lokale Netzwerk nicht erlaubt ist, hiflt ein löschen der App und ein erneuter Installationsversuch. Dies ist besonders wichtig, da viele Netzwerk-Tools (wie UDP-Tests) auf den Zugriff auf das lokale Netzwerk angewiesen sind. Debug-Logs aktiv:Ein Toggle ermöglicht es, den Detailgrad der Log-Ausgaben zu steuern. Wenn der Toggle aktiviert ist, werden Debug- und Trace-Nachrichten zusätzlich ausgegeben – ideal, wenn du in einer Test- oder Support-Situation möglichst viele Details sehen möchtest. Diese Einstellung wird in den UserDefaults gespeichert, sodass sie über App-Neustarts hinweg erhalten bleibt. Log Export Funktion:Die Log-Export-Funktion (die in der Settings-View über einen extra Button – “Logs exportieren” – erreichbar ist) sammelt alle Log-Meldungen, die im DebugLogger und möglicherweise im LogManager protokolliert werden. Diese Logs werden formatiert zusammengefasst, sodass du sie anschließend über ein Share-Sheet (z. B. mit UIActivityViewController) exportieren und zum Beispiel per E-Mail an den Support senden kannst. Vorteile: Du erhältst eine komplette Übersicht aller Debug- und Netzwerk-Events, die in der App auftreten – etwa Verbindungsversuche, Berechtigungsanfragen, Eventualitäten wie „UDP connect() fehlgeschlagen“ und weitere Meldungen. Dies ist besonders nützlich, wenn du bei einem Problem Unterstützung benötigst oder wenn du selbst systematisch Fehler analysieren möchtest. Architektur und Funktionsweisetunneldebugger ist nach dem Model-View-ViewModel (MVVM)-Prinzip aufgebaut: Model/Service:Hier befinden sich alle Klassen, die mit der Netzwerkdiagnose zu tun haben. Beispielsweise steuert der TracerouteManager das Versenden von ICMP-Paketen und verarbeitet die Antworten, während der LocalNetworkManager für die Anfrage von Netzwerkberechtigungen und UDP-Tests zuständig ist. ViewModel:Das InfoViewModel konsolidiert die von den Service-Klassen ermittelten Daten – z. B. Router-IP, Hostname, RTT und WLAN-Daten – und stellt sie als Published‑Eigenschaften zur Verfügung. Änderungen werden automatisch an die Views weitergegeben. View:Die Benutzeroberfläche (z. B. InfoTabView und SettingsTabView) ist so gestaltet, dass sie in einem klassischen Formular (Form) oder in Tabellenübersichten alle relevanten Informationen anzeigt. Die Views sind von der Geschäftslogik entkoppelt, wodurch das Testen und Warten der App wesentlich vereinfacht wird.Fazittunneldebugger bietet dir als IT-Administrator oder technikaffinem Endnutzer ein leistungsfähiges Werkzeug zur Überwachung und Diagnose von VPN- und Netzwerkverbindungen. Dank der MVVM-Architektur ist die App modular und gut testbar, und die Kombination aus Traceroute-, heuristischer und direkter WLAN-Informationsabfrage sorgt dafür, dass du auch in komplexen Netzwerkumgebungen stets die relevanten Informationen zur Hand hast.Wir hoffen, dass dir dieser Leitfaden dabei hilft, die App noch besser zu verstehen und effektiv einzusetzen. Falls du weitere Fragen hast oder Unterstützung benötigst, freuen wir uns über dein Feedback in den Kommentaren." }, { "title": "Apple Content Cache mit Grafana und Prometheus überwachen – Schritt-für-Schritt-Anleitung", "url": "/posts/Apple-Content-Cache-Grafana-Dashboard/", "categories": "Grafana, Apple, Prometheus", "tags": "content-cache, dashboard, monitoring", "date": "2025-03-06 16:52:26 +0100", "snippet": "Apple Content Cache mit Grafana und Prometheus überwachen – Schritt-für-Schritt-AnleitungIntroductionTL;DR: This guide explains why monitoring the Apple Content Cache is crucial for optimizing netw...", "content": "Apple Content Cache mit Grafana und Prometheus überwachen – Schritt-für-Schritt-AnleitungIntroductionTL;DR: This guide explains why monitoring the Apple Content Cache is crucial for optimizing network performance and ensuring efficient content delivery. Follow the step-by-step instructions to install the required software, configure your system, and visualize key metrics.Why Monitor Apple’s Content Caching?Monitoring the Apple Content Cache is essential because it: Optimizes network bandwidth by reducing external traffic. Ensures timely delivery of software updates and apps. Helps identify and resolve performance issues proactively.Apple’s Content Caching optimizes network performance by storing and serving software updates, apps, and other Apple content locally. This reduces bandwidth usage, speeds up downloads, and enhances efficiency across multiple devices. However, ensuring the cache is performing optimally requires proper monitoring—especially in larger environments, where issues like bandwidth bottlenecks or cache misses can impact performance.How Does This Monitoring Stack Work? A custom daemon collects real-time metrics from the Apple Content Cache and exposes them via HTTP (Port 9200). Prometheus scrapes these metrics and stores them for analysis. Grafana visualizes this data, providing insights into cache performance, bandwidth savings, and potential issues.Requirements &amp; Deployment RecommendationsTo run the Apple Content Cache, a Mac is required, as this feature is exclusive to macOS.For deploying the Grafana + Prometheus monitoring stack, you have two options: You can run the Docker Compose stack directly on your Mac using Podman. However, for a more reliable and headless setup, I recommend deploying the stack on a Linux VM or a Raspberry Pi 4 or 5 with at least 4GB of RAM and installed Docker and Docker Compose.By the end of this guide, you’ll have a fully functional monitoring solution that provides real-time insights into the Apple Content Cache on your macOS device. Let’s get started! This compose is a minimal setup, you can add traefik for reverse proxy, and other services like Loki for log aggregation.1. Download and Install the .pkg File from my file server Download the .pkg file and save it to your Mac. Double-click the .pkg file to start the installation process (you need admin privileges because the daemon will launch after boot). Execute the following commands to load and start the daemon: sudo launchctl load /Library/LaunchDaemons/de.cstrube.accpromadapterdaemon.plistsudo launchctl start de.cstrube.accpromadapterdaemon This will install the ACCpromAdapterDaemon on your Mac. This daemon will collect metrics from the Apple Content Cache and expose them to Prometheus over HTTP Port 9200.2. Set Up the Prometheus + Grafana Stack with Docker Compose on your Linux ServerNow, let’s create a Prometheus + Grafana stack using Docker Compose.I assume you have Docker and Docker Compose installed on your server. If not, you can follow the official installation guide for Docker and Docker Compose.Create a new directory for your monitoring stack and navigate to it:sudo mkdir -p /srv/grafana-prometheus-stackcd /srv/grafana-prometheus-stackCreate a new file named docker-compose.yml and paste the following configuration:services: prometheus: container_name: prometheus image: prom/prometheus:latest ports: - 9090:9090 volumes: - ./prometheus:/etc/prometheus - prometheus-data:/prometheus restart: unless-stopped command: --web.enable-lifecycle --config.file=/etc/prometheus/prometheus.yml --storage.tsdb.retention.time=90d alertmanager: container_name: alertmanager image: prom/alertmanager:latest ports: - 9093:9093 volumes: - ./alertmanager:/etc/alertmanager command: - '--config.file=/etc/alertmanager/alertmanager.yml' restart: unless-stopped grafana: image: grafana/grafana:latest container_name: grafana user: \"1000\" volumes: - grafana-data:/var/lib/grafana # - ./config/grafana.ini:/etc/grafana/grafana.ini # Uncomment this line if you want to use a custom configuration file ports: - \"3000:3000\" restart: always env_file: grafana.env mem_limit: 2000m mem_reservation: 300m volumes: prometheus-data: grafana-data:We use the latest versions of Prometheus, Alertmanager, and Grafana in this configuration. You can replace latest with a specific version if needed. With Retention time set to 90 days, you can adjust this value as needed. Also, you can uncomment the grafana.ini line if you want to use a custom configuration file. And we use Docker Volume for Prometheus and Grafana data to persist the data.3. Configure Prometheus to Scrape Metrics from the Apple Content CacheCreate a new directory named prometheus and a new file named prometheus.yml inside it:mkdir prometheuscd prometheustouch prometheus.ymlPaste the following configuration into the prometheus.yml file:global: scrape_interval: 60s # Set the scrape interval to every 60 seconds. Default is every 1 minute. evaluation_interval: 60s # Evaluate rules every 60 seconds. The default is every 1 minute. # scrape_timeout is set to the global default (10s). # Alertmanager configurationalerting: alertmanagers: - static_configs: - targets: - alertmanager:9093# Load rules once and periodically evaluate them according to the global 'evaluation_interval'.rule_files: - \"alerts.yml\" # - \"second_rules.yml\"# A scrape configuration containing exactly one endpoint to scrape:# Here it's Prometheus itself.scrape_configs: # The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config. - job_name: 'prometheus' # metrics_path defaults to '/metrics' # scheme defaults to 'http'. - job_name: 'apple-content-cache' scrape_interval: 60s static_configs: - targets: ['insert_IP_apple_content_cache:9200'] labels: group: 'apple-content-cache-group1' location: 'datacenter-1' hostname: 'hostname1' vlan-id: '100' - job_name: 'more-content-caches' scrape_interval: 60s static_configs: - targets: ['insert_IP_apple_content_cache:9100'] labels: group: 'apple-content-cache-group1' location: 'datacenter-2' hostname: 'hostname2' vlan-id: '200'Replace insert_IP_apple_content_cache with the IP address of your Apple Content Cache. You see in the sample configuration that you can add multiple content caches to scrape metrics from. You can also add more labels or delete labels to the scrape configuration to provide additional context. In larger environments, you can group content caches by location, hostname, or VLAN ID.Optional: Create a file named alerts.yml in the same directory and paste the following configuration:groups: - name: content-cache-alerts rules: - alert: HighRequestsFromClient expr: sum(rate(acc_zrequestsfromclient[5m])) &gt; 100 for: 1m labels: severity: warning annotations: summary: \"High requests from client\" description: \"The number of requests from clients is high.\"This configuration sets up a sample alert that triggers when the number of requests from clients exceeds 100 in a 5-minute window. You can customize this alert or add more alerts as needed. I personaly use discord webhook for alerts, you can use slack or email.cd back to the root directory of the monitoring stack:cd /srv/grafana-prometheus-stackrun the following command to start the monitoring stack:docker-compose up -dYou should see the Prometheus, Alertmanager, and Grafana containers starting up.4. Configure Prometheus to Scrape Metrics from the Apple Content Cache and Troubleshoot Common IssuesNow, let’s configure Prometheus to scrape metrics from the Apple Content Cache.Troubleshooting Tips: Run systemctl status jekyll.service to check the status of the service. Use journalctl -u jekyll.service -xe to review logs and diagnose issues.First, add the prometheus data source to Grafana: Open your browser and navigate to http://your_server_ip:3000. Log in with the default credentials (admin/admin). Click on the gear icon on the left sidebar and select Data Sources. Click on Add data source. Select Prometheus from the list of data sources. In the HTTP section, enter http://prometheus:9090 as the URL. (This is the internal Docker network address of the Prometheus container). Click on Save &amp; Test. You should see a green notification that says Data source is working. Click on the Explore tab to test the Prometheus query.Next, import the pre-built Grafana dashboard for the Apple Content Cache: Open your browser and navigate to http://your_server_ip:3000. Log in with the default credentials (admin/admin). Click on the + icon on the left sidebar and select Import. Download the Dashboard .json file here. Paste the JSON content into the Import via panel json text area. Click on Load. Select the Prometheus data source you added earlier from the dropdown list. Click on Import. You should see the Apple Content Cache Grafana dashboard with metrics visualizations.That’s it! You now have a fully functional monitoring stack to visualize metrics from the Apple Content Cache in Grafana.5. ConclusionIn this guide, we demonstrated how to set up a monitoring solution for the Apple Content Cache using Grafana and Prometheus. We covered installation, configuration, and troubleshooting steps.You can fine-tune the monitoring stack by adding more metrics, alerts, and visualizations to suit your needs. This setup provides real-time insights into the performance of the Apple Content Cache, helping you optimize network bandwidth, identify bottlenecks, and ensure efficient content delivery.More Pictures:" }, { "title": "ACCpromAdapter App – Monitoring und Analyse von Apple Content Cache Metriken", "url": "/posts/swift-ACCpromAdapter-App/", "categories": "Swift, Apple, Cache, Prometheus, Grafana", "tags": "monitoring", "date": "2025-03-02 17:38:03 +0100", "snippet": "ACCpromAdapterVersion: 1.0Author: Christian StrubeLicense: MIT—Die ACCPromAdapter App ist eine innovative macOS-Anwendung, die speziell dafür entwickelt wurde, die Metriken des Apple Content Cache ...", "content": "ACCpromAdapterVersion: 1.0Author: Christian StrubeLicense: MIT—Die ACCPromAdapter App ist eine innovative macOS-Anwendung, die speziell dafür entwickelt wurde, die Metriken des Apple Content Cache zu überwachen und in einem Prometheus-kompatiblen Format bereitzustellen. Dank ihrer flexiblen Architektur können Administratoren und DevOps-Teams die Caching-Infrastruktur bequem visualisieren – etwa in Grafana. Zusätzlich bieten wir mit dem ACCPromAdapterDaemon eine Lösung für headless Installationen an, bei denen der Dienst automatisch beim Systemstart aktiv ist und Metriken auch ohne Benutzeranmeldung sammelt.Was ist ACCPromAdapter?ACCPromAdapter verbindet sich mit dem Apple Content Cache und liest die Metriken aus der zugehörigen Metrics.db aus. Die App entscheidet intelligent, ob sie die Metriken von einem extern laufenden Daemon (HTTP-Server) oder über den direkten Zugriff auf die lokale Datenbank bezieht. Diese duale Strategie stellt sicher, dass immer aktuelle Daten zur Verfügung stehen – unabhängig davon, ob der externe Dienst verfügbar ist oder nicht.Hauptfunktionen Automatische Umgebungserkennung: Beim Start prüft die App, ob ein externer HTTP-Daemon aktiv ist, der die Metriken bereits bereitstellt. Falls ja, werden diese genutzt; andernfalls greift die App auf die lokale Metrics.db zu. Prometheus-kompatibler Output: Die Metriken werden in einem Format ausgegeben, das direkt von Prometheus abgefragt werden kann. So lässt sich die App nahtlos in Grafana-Dashboards integrieren. Benutzerfreundliche Menüleisten-Anzeige: Ein übersichtliches UI in der macOS-Menüleiste zeigt die wichtigsten Metriken an – ergänzt durch einen visuellen Statusindikator, der den aktiven Modus (extern oder lokal) anzeigt. Automatische Aktualisierung: Die Metriken werden in regelmäßigen 30-Sekunden-Zyklen aktualisiert, wobei ein Fortschrittsbalken den aktuellen Stand des Updates visuell darstellt. Headless Installation mit ACCPromAdapterDaemon: Für Umgebungen ohne Benutzeranmeldung, wie z. B. Server oder Headless Mac Minis, bietet der ACCPromAdapterDaemon eine vollständig automatisierte Lösung. Der Daemon startet beim Systemstart und liefert kontinuierlich Metriken, die von Prometheus abgefragt werden können.Technische ArchitekturDie App ist in Swift geschrieben und nutzt moderne Technologien wie Swift Concurrency (async/await), Combine und SwiftNIO für die Netzwerkkommunikation. Dabei wird ein modularer Ansatz verfolgt: AppInitializer: Verantwortlich für die zentrale Initialisierung der Anwendung und die Entscheidung, ob externe oder lokale Metriken verwendet werden. MetricsCache: Sammelt und aktualisiert die Metriken aus der lokalen Datenbank, falls kein externer Daemon verfügbar ist. MetricsFetcher: Holt die Metriken über HTTP, wenn der externe Daemon aktiv ist. PrometheusServer: Startet einen internen HTTP-Server, der Prometheus-kompatiblen Output liefert – als Fallback, wenn kein externer Dienst vorhanden ist. ACCPromAdapterDaemon: Ein separater Daemon, der speziell für headless Installationen konzipiert wurde. Er läuft systemweit, sammelt Metriken aus der Metrics.db und stellt sie über einen HTTP-Endpunkt bereit, ohne dass ein Benutzer eingeloggt sein muss.EinsatzmöglichkeitenACCPromAdapter eignet sich ideal für: Administrator:innen und IT-Teams, die die Performance und Auslastung ihres Apple Content Cache überwachen möchten. DevOps-Teams, die Dashboards in Grafana erstellen wollen, um Metriken in Echtzeit zu visualisieren. Headless Umgebungen, in denen der ACCPromAdapterDaemon dafür sorgt, dass Metriken automatisch auch ohne Benutzeranmeldung gesammelt werden.Installation &amp; Erste Schritte ACCPromAdapter App: Laden Sie die App direkt bei Github herunter. Beim ersten Start werden Sie aufgefordert, die Metrics.db auszuwählen, um den Lesezugriff zu konfigurieren. ACCPromAdapterDaemon (Optional): Laden Sie das Installationspaket (ACCpromAdapterDaemon.pkg) herunter – verfügbar über GitHub oder als Direktdownload im Blog. Installieren Sie den Daemon, der beim Systemstart automatisch aktiviert wird. Monitoring: Starten Sie die App und beobachten Sie die Metriken in der Menüleiste. Falls der externe Daemon aktiv ist, werden die Metriken von diesem abgerufen. Grafana-Integration: Konfigurieren Sie Prometheus so, dass es die Metriken von http://localhost:9200/metrics abruft, und erstellen Sie ein passendes Dashboard in Grafana.FazitACCPromAdapter und der ACCPromAdapterDaemon bieten zusammen eine flexible und robuste Lösung zur Überwachung der Apple Content Cache Metriken. Mit der dualen Strategie – ob über einen externen HTTP-Daemon oder direkt aus der lokalen Datenbank – garantiert die Lösung stets aktuelle und zuverlässige Daten. Dieses innovative Tool hilft Ihnen, die Performance Ihrer Caching-Infrastruktur optimal zu überwachen und Probleme frühzeitig zu erkennen.Bleiben Sie dran für weitere Updates und Verbesserungen der ACCPromAdapter App!Für Fragen und Feedback stehe ich Ihnen jederzeit zur Verfügung – kontaktieren Sie mich über GitHub oder per E-Mail.English Version:DescriptionACCPromAdapter is an innovative macOS menu bar application designed to monitor the Apple Content Cache metrics and provide them in a Prometheus-compatible format. The app reads the Metrics.db file from the system directory and serves the data via an integrated HTTP server, allowing administrators and DevOps teams to seamlessly visualize the caching infrastructure in tools such as Grafana.In addition, we offer the ACCPromAdapterDaemon for headless installations. This daemon is ideal for server environments and headless Mac Minis where the service automatically starts at system boot and collects metrics even without a user login.What is ACCPromAdapter?ACCPromAdapter connects to the Apple Content Cache and reads the metrics from its associated Metrics.db file. The app intelligently determines whether to retrieve the metrics from an externally running daemon (HTTP server) or by directly accessing the local database. This dual approach ensures that current data is always available, regardless of whether the external service is operational.Key Features Automatic Environment Detection: On startup, the app checks if an external HTTP daemon is active to provide metrics. If so, these metrics are used; otherwise, the app falls back to the local Metrics.db. Prometheus-Compatible Output: The metrics are output in a format that can be directly scraped by Prometheus, enabling seamless integration into Grafana dashboards. User-Friendly Menu Bar Interface: A clear UI in the macOS menu bar displays the key metrics, complemented by a visual status indicator showing the active mode (external or local). Automatic Updates: Metrics are refreshed every 30 seconds, with a progress bar visually indicating the current update cycle. Headless Installation with ACCPromAdapterDaemon: For environments without user interaction—such as servers or headless Mac Minis—the ACCPromAdapterDaemon provides a fully automated solution. The daemon starts at system boot and continuously serves metrics that Prometheus can scrape.Technical ArchitectureThe app is written in Swift and leverages modern technologies such as Swift Concurrency (async/await), Combine, and SwiftNIO for network communication. It employs a modular architecture: AppInitializer: Handles the central initialization of the application and determines whether to use external HTTP metrics or the local database. MetricsCache: Collects and updates metrics from the local database if no external daemon is available. MetricsFetcher: Retrieves metrics over HTTP when an external daemon is active. PrometheusServer: Launches an internal HTTP server that delivers Prometheus-compatible output as a fallback when no external service is present. ACCPromAdapterDaemon: A separate daemon designed specifically for headless installations. It runs system-wide, gathers metrics from Metrics.db, and exposes them via an HTTP endpoint without requiring a user login.Use CasesACCPromAdapter is ideal for: Administrators and IT Teams who need to monitor the performance and load of their Apple Content Cache. DevOps Teams who wish to build Grafana dashboards for real-time metrics visualization. Headless Environments where the ACCPromAdapterDaemon ensures continuous metrics collection even without user interaction.Installation &amp; Getting StartedACCPromAdapter App: Clone the Project: git clone https://github.com/MadChristian/ACCpromAdapter.gitcd ACCpromAdapter Open in Xcode: open ACCpromAdapter.xcodeproj Set Up Code Signing: In Xcode, navigate to “Signing &amp; Capabilities” and enter your Developer ID as required. Run the App: Press Cmd + R to build and launch the app. Initial Configuration: On first launch, you will be prompted to select the Metrics.db file to configure read access. ACCPromAdapterDaemon (Optional): - Download the installation package ACCPromAdapterDaemon.pkg from GitHub Releases or as a direct download on the blog. - Install the daemon, which will automatically start at system boot. Monitoring: Launch the app and monitor the metrics in the menu bar. If the external daemon is active, metrics will be retrieved via HTTP.Grafana Integration: Configure Prometheus to scrape metrics from:http://localhost:9200/metricsThen, create a suitable dashboard in Grafana.ConclusionACCPromAdapter and the ACCPromAdapterDaemon together provide a flexible and robust solution for monitoring Apple Content Cache metrics. With a dual strategy—retrieving metrics via an external HTTP daemon or directly from the local database—this innovative tool ensures that up-to-date and reliable data is always available. This empowers you to optimally monitor your caching infrastructure and identify potential issues early on.Stay tuned for further updates and improvements to the ACCPromAdapter App!DeploymentThe latest version of the ACCpromAdapter App and the daemon is available as a release. Visit the Releases Page to download the installation package.For any questions or feedback, please contact me via GitHub or by email." }, { "title": "Update - Ansible Semaphore", "url": "/posts/ansible-semaphore-updates/", "categories": "Automation, Scripting", "tags": "nginx, webserver, jekyll, shell, bash, ansible, semaphore", "date": "2024-04-14 21:00:00 +0200", "snippet": "Integration von Ansible in Semaphore für Automatisierte Deployment-Prozesse Reboots mit Ansible und Semaphore: Du kannst Ansible in Semaphore nutzen, um Server neu zu starten und zu überprüfen...", "content": "Integration von Ansible in Semaphore für Automatisierte Deployment-Prozesse Reboots mit Ansible und Semaphore: Du kannst Ansible in Semaphore nutzen, um Server neu zu starten und zu überprüfen, ob diese Neustarts erfolgreich waren. Dies kann durch das ansible.builtin.reboot Modul erfolgen, das nicht nur die Maschinen neu startet, sondern auch sicherstellt, dass sie nach dem Neustart wieder ordnungsgemäß funktionieren. Verwendung von Datenbanken für Zustandsmanagement: Während Semaphore eine interne Datenbank für operationelle Zwecke nutzt, ist es empfehlenswert, für benutzerdefinierte Daten und Zustände eine separate Datenbank zu verwenden. Dies hilft, die Integrität der Semaphore-Datenbank zu bewahren und bietet dir mehr Flexibilität und Kontrolle über deine eigenen Daten. Sicheres Management von Konfigurationsdaten: Das Speichern von sensiblen Informationen wie Datenbankverbindungsdaten sollte sicher erfolgen. Wir haben verschiedene Methoden besprochen: Umgebungsvariablen: Eine gängige Methode zur Laufzeitkonfiguration, die das Speichern sensibler Daten außerhalb des Codes ermöglicht. Docker Secrets: Bietet eine sichere Speicherung und Verwaltung von Geheimnissen, ideal für den Einsatz in Docker Swarm Umgebungen. Externe Secrets Manager: Tools wie HashiCorp Vault oder AWS Secrets Manager bieten erweiterte Funktionen für das Secrets Management, insbesondere in komplexen Umgebungen. Erweiterte Nutzung von Ansible Playbooks: Zur Automatisierung und Überwachung von Server-Management-Aufgaben können Ansible Playbooks verwendet werden, um Zustände wie Festplattennutzung zu prüfen und Aktionen wie das Senden von Benachrichtigungen über Discord zu automatisieren.Praktische Anwendungsfälle Automatisierte Reboots: Nutze Ansible, um Server basierend auf bestimmten Kriterien (z.B. nach Updates) automatisch neu zu starten. Überwachung und Benachrichtigungen: Setze Playbooks ein, um Systemzustände zu überwachen und bei Bedarf Benachrichtigungen zu senden, etwa wenn der Festplattenspeicher knapp wird. Sicheres Config Management: Implementiere robuste Mechanismen für das Management von Konfigurationsdaten und Geheimnissen, um die Sicherheit und Compliance zu gewährleisten.Diese Strategien und Tools ermöglichen eine effiziente Verwaltung von IT-Infrastrukturen, automatisieren Routineaufgaben und erhöhen die Sicherheit durch sorgfältige Handhabung sensibler Daten. Nutze die Stärken von Ansible, Semaphore und Docker, um deine Deployment- und Operations-Prozesse zu optimieren.Automatisierung von Docker Compose mit Ansible Docker Compose und Ansible: Wir haben besprochen, wie man Ansible zusammen mit Docker Compose verwendet, um Dienste automatisch zu verwalten. Dies ist besonders nützlich, wenn regelmäßige Updates oder Neustarts von Services wie BIND9 erforderlich sind, die kritische Infrastrukturkomponenten darstellen. Ansible Playbook zur Serviceverwaltung: Du kannst ein Ansible Playbook verwenden, um Docker Compose Services zu steuern. Hierbei wird das community.docker.docker_compose_v2 Modul eingesetzt, um spezifische Dienste wie adblock und bind9 in einem Docker Compose-Projekt neu zu starten. Das Playbook ermöglicht es, den Zustand dieser Dienste zu kontrollieren und sicherzustellen, dass sie nach der Aktualisierung von Konfigurationen oder Daten korrekt laufen. Beispiel-Playbook:```yaml— name: Manage Docker Compose Services at /home/christian/bind9-dnshosts: allbecome: truetasks: name: Ensure Docker Compose application is runningcommunity.docker.docker_compose_v2: project_src: “/home/christian/bind9-dns” state: restarted services: - adblock - bind9register: output name: Send Discord message on successuri: url: “” method: POST body_format: json body: ‘{“content”: “Docker Compose services at /home/christian/bind9-dns have been successfully restarted on !”}’ headers: Content-Type: “application/json” status_code: 204when: output.changed``` Benachrichtigungen: Im Anschluss an die Neustartung der Services haben wir auch die Integration von Benachrichtigungen mittels Discord besprochen. Dies ermöglicht es, automatisierte Rückmeldungen über den Status der Neustartprozesse zu erhalten.Integration und Überwachung Monitoring und Feedback: Durch die Integration von Feedback-Mechanismen wie Discord-Benachrichtigungen kannst du den Zustand der Infrastruktur proaktiv überwachen und bei Bedarf schnell reagieren. Automatisierung von Wartungsprozessen: Die Kombination aus Ansible und Docker Compose bietet eine leistungsstarke Methode zur Automatisierung von Wartungs- und Update-Prozessen, die regelmäßige Interventionen erfordern, wie das Aktualisieren von DNS RPZ-Zonen in BIND9.Diese Techniken verbessern nicht nur die Effizienz der Systemverwaltung, sondern auch die Zuverlässigkeit und Reaktionsfähigkeit der IT-Infrastruktur. Indem du diese Prozesse automatisierst, minimierst du das Risiko von menschlichen Fehlern und stellst sicher, dass kritische Dienste stets aktuell und funktional sind." }, { "title": "Parken App Updates 0.0.6: Editiermodus, Zahlenschloss und Historienansicht", "url": "/posts/swift-parken-app-0-0-6/", "categories": "Swift, App", "tags": "swiftui, iOS, backend", "date": "2023-08-13 21:12:53 +0200", "snippet": "Parken App Updates 0.0.6: Editiermodus, Zahlenschloss und HistorienansichtHallo liebe Parken-App-Nutzer! Ich freue mich, euch in diesem Beitrag einige aufregende neue Funktionen in der neuesten Ver...", "content": "Parken App Updates 0.0.6: Editiermodus, Zahlenschloss und HistorienansichtHallo liebe Parken-App-Nutzer! Ich freue mich, euch in diesem Beitrag einige aufregende neue Funktionen in der neuesten Version 0.0.6 der Parken App vorstellen zu dürfen. Diese Verbesserungen machen die App nicht nur benutzerfreundlicher, sondern auch sicherer und informativer.EditiermodusEine der bemerkenswertesten Neuerungen ist der Editiermodus. Es ermöglicht dem Benutzer, bestimmte Fahrzeugdetails wie das Kennzeichen, die FIN/VIN oder den Status des Fahrzeugs zu ändern.Um den Editiermodus zu aktivieren, tippen Sie einfach auf einen der Einträge, und Sie werden in den Bearbeitungsmodus versetzt, in dem Sie die gewünschten Änderungen vornehmen können..onTapGesture(count: 1) {\tself.editMode = true}Ein langer Druck auf den Bildschirm speichert die Änderungen und beendet den Editiermodus:.onLongPressGesture {\tif self.editMode {\t\tsaveChanges()\t}}Zahlenschloss für den SchlüsselEine der interessantesten Neuerungen ist die Einführung eines Zahlenschlosses für den Schlüssel. Diese Funktion ermöglicht es dem Benutzer, über ein einfaches Picker-Interface eine vierstellige Nummer für den Schlüssel festzulegen:if title == \"Schlüssel Nr.:\" {\tHStack {\t\tdigitPicker(binding: $firstDigit)\t\tdigitPicker(binding: $secondDigit)\t\tdigitPicker(binding: $thirdDigit)\t\tdigitPicker(binding: $fourthDigit)\t}}HistorienansichtDie Parken App bietet jetzt eine Historienansicht, die eine Liste der verschiedenen Statusänderungen für das Fahrzeug anzeigt. Dies ist besonders nützlich, um einen Überblick über die verschiedenen Aktivitäten und Änderungen zu erhalten, die im Laufe der Zeit am Fahrzeug vorgenommen wurden.Ein weiteres Highlight dieser Funktion ist die Paginierung, die für eine nahtlose Benutzererfahrung bei der Durchsicht der Historie sorgt.VStack {\t// Griff\tRoundedRectangle(cornerRadius: 3)\t\t.fill(Color.gray)\t\t.frame(width: 50, height: 5)\t\t.padding(8)\tText(\"Historie\")\t\t.font(.headline)\t\t.padding(.bottom, 8)\tHistoryView(viewModel: HistoryViewModel(vehicleId: viewModel.vehicle.id))}AbschlussMit diesen neuen Funktionen hoffen wir, dass Ihr Fahrzeugmanagement mit der Parken App noch effizienter und angenehmer wird. Wir arbeiten ständig daran, die App zu verbessern und sind gespannt auf Ihr Feedback!" }, { "title": "Parken App Updates 0.0.5", "url": "/posts/swift-parken-app-0-0-5/", "categories": "Swift, App", "tags": "swiftui, iOS, backend", "date": "2023-07-31 00:23:49 +0200", "snippet": "Liebes Tester Team,in dieser Version sind einige Bugs gefixt worden. App Abstürze aus dem Background heraus sind behoben worden.Folgende Neuerungen sind implementiert worden: parken: oben wird jet...", "content": "Liebes Tester Team,in dieser Version sind einige Bugs gefixt worden. App Abstürze aus dem Background heraus sind behoben worden.Folgende Neuerungen sind implementiert worden: parken: oben wird jetzt die aktive Gruppe angezeigt, der Kennzeichenscanner vergleicht den Anfang des Kennzeichens mit der Liste aller möglichen Zulassungsbezirke, das sollte die Genauigkeit erhöhen. Das Scannen allgemein geht jetzt wesentlich schneller. Kartenansicht: Die Karte ist jetzt beim ersten öffnen auf Deutschland gezoomt, bei jedem weiteren öffnen auf das zuletzt hinzugefügte Fahrzeug Fahrzeuge: In der Detailsansicht können jetzt Kennzeichen, FIN und Status angepasst werden, beim klicken auf speichern werden die Daten auf dem Server aktualisiert. Wenn das erfolgreich war werden die Daten auch auf dem Gerät aktualisiert. Profil: Gruppeneinladungen und Gruppe verlassen funktionieren jetzt wie vorgesehen.Viel Spaß beim Ausprobieren, ich freue mich auf euer Feedback" }, { "title": "Parken App und Backend - Updates und Bugfixes", "url": "/posts/swift-parken-app-0-0-4/", "categories": "Swift, App", "tags": "iOS, backend", "date": "2023-07-26 01:03:36 +0200", "snippet": "Wir freuen uns, Ihnen die neuesten Aktualisierungen und Bugfixes für unsere Parken App und das zugehörige Backend vorstellen zu können. Wir arbeiten kontinuierlich daran, die Benutzererfahrung zu v...", "content": "Wir freuen uns, Ihnen die neuesten Aktualisierungen und Bugfixes für unsere Parken App und das zugehörige Backend vorstellen zu können. Wir arbeiten kontinuierlich daran, die Benutzererfahrung zu verbessern und Probleme zu beheben, die sich auf die Leistung auswirken können. Hier ist, was in unserem jüngsten Update enthalten ist:Parken App1. Fahrzeugstatus Filter: Wir haben eine neue Funktion hinzugefügt, die es den Benutzern ermöglicht, nach dem Fahrzeugstatus zu filtern. Jetzt können Sie einfach nach Fahrzeugen suchen, die bestimmte Statuskriterien erfüllen.2. Suchfeld Verbesserungen: Die Suche ist jetzt nicht mehr case-sensitiv und unterstützt die Suche nach Kennzeichen und VIN (Vehicle Identification Number). Die Suchergebnisse aktualisieren sich jetzt in Echtzeit, während Sie tippen.3. Fehlerbehebung: Wir haben einen Absturz behoben, der auftrat, wenn auf einen Speicherbereich zugegriffen wurde, der nicht existiert. Dies wurde durch verbessertes Speicher- und Thread-Management behoben.4. Performance-Optimierungen: Um die App reaktionsschneller zu machen, haben wir die Performance verbessert, insbesondere beim Laden und Anzeigen der Fahrzeugliste.Backend1. Datenvalidierung: Wir haben unsere Datenvalidierung verbessert, um sicherzustellen, dass alle eingehenden Daten korrekt formatiert sind. Dies wird dazu beitragen, Fehler und Inkonsistenzen in der Datenbank zu reduzieren.2. API Optimierung: Die Performance unserer Backend-API wurde verbessert. Jetzt werden die Anfragen schneller bearbeitet, was zu einer verbesserten Benutzererfahrung führt.3. Datenbanksicherheit: Wir haben unsere Datenbanksicherheitsmaßnahmen aktualisiert, um die Daten unserer Benutzer noch besser zu schützen.4. Fehlerprotokollierung: Die Fehlerprotokollierung wurde verbessert, um uns zu helfen, Probleme schneller zu erkennen und zu beheben.Wir sind stolz auf die Verbesserungen, die wir in diesem Update vorgenommen haben, und wir freuen uns darauf, die Parken App und das Backend weiter zu verbessern, um unseren Benutzern den besten Service zu bieten. Wie immer, wenn Sie Feedback oder Vorschläge haben, würden wir uns freuen, von Ihnen zu hören." }, { "title": "Apple Push Notification Service mit Quart und Python: Ein Leitfaden", "url": "/posts/swift-apns/", "categories": "Programming, Python", "tags": "apns", "date": "2023-07-22 00:47:44 +0200", "snippet": "Echtzeitbenachrichtigungen sind ein wesentlicher Bestandteil moderner Anwendungen, um die Benutzererfahrung zu verbessern und die Benutzerbindung zu erhöhen. In diesem Blogbeitrag werde ich erkläre...", "content": "Echtzeitbenachrichtigungen sind ein wesentlicher Bestandteil moderner Anwendungen, um die Benutzererfahrung zu verbessern und die Benutzerbindung zu erhöhen. In diesem Blogbeitrag werde ich erklären, wie man den Apple Push Notification Service (APNs) mit Quart, einem asynchronen Python-Web-Framework, einrichtet und verwendet.VoraussetzungenBevor wir beginnen, stellen Sie sicher, dass Sie Folgendes haben: Ein APNs-Zertifikat, das von Apple ausgestellt wurde. Die Bundle-ID Ihrer App. Die Gerätekennung für jedes Gerät, das Benachrichtigungen empfangen soll.Quart-SetupZunächst einmal installieren wir Quart und aioapns, eine asynchrone APNs-Bibliothek für Python, indem wir den folgenden Befehl ausführen:pip install quart aioapnsJetzt, da wir Quart und aioapns installiert haben, können wir beginnen, unseren Code zu schreiben. Zunächst einmal erstellen wir eine neue Quart-App und definieren einige Konfigurationsvariablen.from quart import Quart, request, jsonify, make_responsefrom aioapns import APNs, Client, NotificationRequest, PushTypeapp = Quart(__name__)# Setzen Sie die Konfigurationsvariablenapp.config['APNS_KEY_FILE'] = '/path/to/your/key.pem' # Pfad zur Schlüsseldateiapp.config['APNS_CERT_FILE'] = '/path/to/your/cert.pem' # Pfad zur Zertifikatsdateiapp.config['APNS_TOPIC'] = 'com.yourcompany.yourapp' # Die Bundle-ID Ihrer AppEinrichten des APNs-ClientsNun, da wir unsere App konfiguriert haben, können wir den APNs-Client einrichten.@app.before_servingasync def setup_apns_client(): client = APNs( client_cert=app.config['APNS_CERT_FILE'], client_key=app.config['APNS_KEY_FILE'], use_sandbox=False, # Setzen Sie dies auf True, wenn Sie den Sandbox-APNs-Server verwenden ) app.config['APNS_CLIENT'] = clientAPI-Route zum Aktualisieren der Device-TokenWir werden eine API-Route erstellen, die es Clients erlaubt, ihre Device-Token zu aktualisieren.@app.route('/update-device-token', methods=['POST'])async def update_device_token(): data = await request.get_json() device_token = data.get('device_token') # Hier würden Sie normalerweise den Token in Ihrer Datenbank speichern. # Zum Zwecke dieses Tutorials drucken wir ihn einfach aus. print(f\"Received device token: {device_token}\") return make_response(jsonify({'message': 'Device token updated successfully'}), 200)Senden von Push-BenachrichtigungenJetzt können wir eine Funktion erstellen, um Push-Benachrichtigungen zu senden.async def send_push_notification(device_token, message): client = app.config['APNS_CLIENT'] notification_request = NotificationRequest( device_token=device_token, message={ \"aps\": { \"alert\": message, \"badge\": 1, } }, push_type=PushType.ALERT, ) try { response = await client.send_notification(notification_request) if response.is_successful: print(f\"Notification sent successfully to {device_token}\") else: print(f\"Failed to send notification: {response.error_reason}\") } except Exception as e { print(f\"An error occurred while sending the notification: {str(e)}\") }Jetzt können Sie die send_push_notification-Funktion verwenden, um eine Push-Benachrichtigung an ein bestimmtes Gerät zu senden. Geben Sie einfach den Gerätetoken und die Nachricht, die Sie senden möchten, als Argumente an.Und das war’s! Sie haben jetzt einen lauffähigen Quart-Server, der Push-Benachrichtigungen über den Apple Push Notification Service senden kann. Beachten Sie, dass dieser Code nur für Demonstration und Bildungszwecke gedacht ist und Sie ihn entsprechend Ihren Anforderungen anpassen und erweitern sollten, insbesondere im Hinblick auf Fehlerbehandlung und Sicherheit." }, { "title": "Künstliche Intelligenz und ihre Auswirkungen auf die Arbeitswelt: Eine Perspektive basierend auf LLM", "url": "/posts/ai-llm/", "categories": "AI", "tags": "künstlicheIntelligenz,, arbeitswelt,, llm", "date": "2023-07-10 16:16:28 +0200", "snippet": "Künstliche Intelligenz (KI) durchläuft gerade eine beeindruckende Evolution, die unser Leben und unsere Arbeitswelt nachhaltig verändert. Ein entscheidendes Konzept in dieser Entwicklung ist das Le...", "content": "Künstliche Intelligenz (KI) durchläuft gerade eine beeindruckende Evolution, die unser Leben und unsere Arbeitswelt nachhaltig verändert. Ein entscheidendes Konzept in dieser Entwicklung ist das Lernen durch Latent Logic Modeling (LLM), ein fortschrittlicher Ansatz zum Erlernen komplexer Verhaltensweisen.LLM ist ein spezielles Gebiet des maschinellen Lernens, das darauf abzielt, Maschinen ein besseres Verständnis für die zugrunde liegende Logik von Daten zu vermitteln. Es basiert auf der Theorie, dass alle Verhaltensmuster - auch die komplexesten - in einer verborgenen, latenten Logik wurzeln. Mit anderen Worten, LLM versucht, die “Gedanken” hinter den Handlungen zu entschlüsseln.Wie wirkt sich das auf die Arbeitswelt aus? Künstliche Intelligenz wird zunehmend in vielen Bereichen eingesetzt, darunter Automatisierung, Datenanalyse, Entscheidungsfindung, Produktentwicklung und Kundenbetreuung. Die Integration von KI in diese Bereiche hat dazu geführt, dass Arbeitsprozesse effizienter, genauer und produktiver werden.Das LLM verbessert die KI-Fähigkeiten noch weiter. Durch das Verständnis der latenten Logik hinter Verhaltensmustern können KI-Systeme nicht nur wiederholbare Aufgaben ausführen, sondern auch adaptive und proaktive Maßnahmen ergreifen. Dies bedeutet, dass sie in der Lage sind, auf Veränderungen zu reagieren, Entscheidungen zu treffen und in gewissem Maße selbstständig zu agieren.Was bedeutet das für uns Menschen im Arbeitsumfeld? Zum einen können wir uns auf eine Steigerung der Produktivität freuen. KI-Systeme, die mit LLM ausgestattet sind, können repetitive und zeitaufwändige Aufgaben übernehmen, sodass wir uns auf komplexere und kreativere Aspekte unserer Arbeit konzentrieren können.Zum anderen stellt es uns vor neue Herausforderungen. Mit der zunehmenden Einführung von KI am Arbeitsplatz müssen wir lernen, mit KI-Systemen effektiv zusammenzuarbeiten und die richtigen Fähigkeiten entwickeln, um in einer zunehmend automatisierten Arbeitswelt wettbewerbsfähig zu bleiben.Ein weiterer Aspekt ist die ethische Dimension. Mit der zunehmenden Fähigkeit von KI-Systemen, Entscheidungen zu treffen, müssen wir klare Richtlinien und Regeln für die ethische Verwendung von KI festlegen. Dies umfasst Themen wie Verantwortlichkeit, Datenschutz und Fairness.Abschließend kann gesagt werden, dass Künstliche Intelligenz, insbesondere durch Fortschritte im LLM, das Potenzial hat, unsere Arbeitswelt tiefgreifend zu verändern. Es liegt an uns, diese Veränderungen positiv zu gestalten und eine Zukunft zu schaffen, in der KI und Mensch im Arbeitsumfeld harmonisch zusammenarbeiten.Image:DALL-E: Text für das Image kam von Chat GPT" }, { "title": "Blog Update: Automatisierung der Bildoptimierung und WebP-Konvertierung", "url": "/posts/blog-update/", "categories": "Blog, Update", "tags": "update, jekyll, bash, script, webp, jpegoptim, cwebp, base64", "date": "2023-07-09 23:47:49 +0200", "snippet": "Wir haben ein Bash-Skript entwickelt und eingerichtet, das inotify verwendet, um neu hinzugefügte Bilder zu überwachen. Sobald ein neues Bild erkannt wird, optimiert das Skript das Bild mit jpegopt...", "content": "Wir haben ein Bash-Skript entwickelt und eingerichtet, das inotify verwendet, um neu hinzugefügte Bilder zu überwachen. Sobald ein neues Bild erkannt wird, optimiert das Skript das Bild mit jpegoptim, konvertiert es dann mit cwebp in das WebP-Format und erzeugt eine LQIP-Version des Bildes für eine verbesserte Seitendarstellung während des Ladens. Erstellung von Markdown-Posts automatisieren: Das Skript generiert zudem automatisch eine Markdown-Datei für jeden neuen Blog-Post, die in Jekyll verwendet wird. Diese Datei enthält Header-Informationen sowie Links zu den optimierten Bildern. Implementierung von inotify für Dateilöschungen: Wir haben versucht, inotifywait so einzurichten, dass es auf gelöschte Dateien reagiert und einen rsync-Befehl ausführt, um Änderungen zwischen lokalen und entfernten Verzeichnissen zu synchronisieren. Allerdings sind wir auf einige Herausforderungen gestoßen und haben letztendlich entschieden, syncthing für die Synchronisation zu verwenden. Einrichten von Syncthing für Dateisynchronisation: Wir haben Syncthing auf dem Server installiert und konfiguriert, um Änderungen im Blog-Post-Verzeichnis in Echtzeit zu überwachen und zu synchronisieren. Dies hat den Vorteil, dass es bidirektional arbeitet und Dateiänderungen sowohl lokal als auch auf dem entfernten Server überwacht. Zusammengefasst haben wir heute einen signifikanten Automatisierungsprozess implementiert, der die Arbeit mit Bildern in Jekyll-Posts stark vereinfacht. Dies wird nicht nur die Qualität und Performance der Website verbessern, sondern auch viel Zeit sparen, die sonst für manuelles Optimieren, Konvertieren und Hochladen der Bilder aufgewendet werden müsste." }, { "title": "Scripting automate new jekyll-posts", "url": "/posts/scripting-automate-new-jekyll-posts/", "categories": "Programming, Scripting", "tags": "nginx, webserver, jekyll, automation, scripting, shell, bash", "date": "2023-07-08 01:00:00 +0200", "snippet": "This script aims to automate the process of creating a new blog post in Jekyll, specifically optimizing images and generating their associated Base64 Low Quality Image Placeholders (LQIP).Here’s an...", "content": "This script aims to automate the process of creating a new blog post in Jekyll, specifically optimizing images and generating their associated Base64 Low Quality Image Placeholders (LQIP).Here’s an overview of the script and its functions:1. Preparation:Initially, the script defines necessary directories and ensures they exist. It creates them if they don’t exist.2. Directory Monitoring:It uses inotifywait to monitor the directory where new images are uploaded for new files. It responds to “create” and “moved_to” events and processes every new .jpg file added to the directory.3. Post Creation:For each uploaded image, a new blog post is created based on the image’s directory name. The script generates a Jekyll-compatible markdown file with metadata like title, date, categories, tags, and author. It also creates a special image metadata field that contains the path to the image file and the Base64 string of the LQIP.4. Image Optimization:Each image is copied and optimized to achieve a smaller file size. After that, the optimized image is converted to the WebP format and moved to a special directory accessible by the Nginx webserver.5. LQIP Generation:In addition, a low-resolution version of the image is created, converted to Base64, and stored in a file. This is used to generate the LQIPs for the blog post, which are displayed during the page load before the actual image is loaded.6. Cleanup:After all steps are successfully completed, the script deletes the temporary and optimized images.7. Synchronization:Finally, the script uses syncthing, to sync the generated Base64 files and blog post files to a remote server.All in all, this script serves to optimize and automate the process of creating blog posts. It reduces the effort associated with manual image optimization and LQIP creation and simplifies the creation of blog posts in Jekyll.Here is the full script. Remember that you will need to adjust the variables and paths to suit your own environment:#!/bin/bashdir=/path/to/importbase64dir=$dir/path/to/base64imagedir=/path/to/public/imagespostsdir=/path/to/postsremote_dir=user@jekyll-server:~/path/to/newpostsecho \"Script started.\" &gt; /var/log/webp-converter/info.logmkdir -p $base64dir $imagedir $postsdir || { echo \"Failed to create directories\"; exit 1; }image_count=0echo \"Image count is now $image_count\" &gt;&gt; /var/log/webp-converter/info.loginotifywait -m $dir -e create -e moved_to --format '%w%f' -r | while read file; do\techo \"New File detected: $file\" &gt;&gt; /var/log/webp-converter/info.log relative_path=${file#$dir/} if [[ $file =~ .jpg$ ]] &amp;&amp; [[ $file != *_optimized.jpg ]] &amp;&amp; [[ $file != .* ]]; then echo \"Processing $file...\" &gt;&gt; /var/log/webp-converter/info.log\t\t postname=$(basename $(dirname $file)) filename=$(basename $file .jpg) postdir=$imagedir/$postname mkdir -p $postdir || { echo \"Failed to create directory $postdir\"; continue; } mdfile=$postsdir/$(date +'%Y-%m-%d')-${postname}.md optimized=$dir/${relative_path%.*}_optimized.jpg rsync -avP $file $optimized || { echo \"Failed to copy $file\"; continue; } jpegoptim -s $optimized || { echo \"Failed to optimize $file\"; continue; } webp=$postdir/${filename}.webp cwebp -q 80 $optimized -o $webp || { echo \"Failed to convert $file to WebP\"; continue; } base64file=$base64dir/${filename}_optimized.base64 convert $optimized -resize 20 - | base64 | tr -d '\\n' &gt; $base64file || { echo \"Failed to convert $file to Base64\"; continue; } image_path=\"https://images.example.com/blog/$postname/${filename}.webp\" base64_string=$(cat $base64file) image_count=$(grep -c \"image:\" $mdfile)\t\techo \"Image count is now $image_count\" &gt;&gt; /var/log/webp-converter/info.log if [[ $image_count -eq 0 ]]; then\t\techo \"Writing header to $mdfile\" &gt;&gt; /var/log/webp-converter/info.log echo \"---\" &gt; $mdfile || { echo \"Failed to create $mdfile\"; continue; } echo \"layout: post\" &gt;&gt; $mdfile\t\techo \"title: \\\"Title Here\\\"\" &gt;&gt; $mdfile echo \"date: $(date +'%Y-%m-%d %H:%M:%S %z')\" &gt;&gt; $mdfile echo \"categories: category1 category2\" &gt;&gt; $mdfile echo \"tags: tag1 tag2\" &gt;&gt; $mdfile echo \"image:\" &gt;&gt; $mdfile || { echo \"Failed to write to $mdfile\"; continue; } echo \" path: $image_path\" &gt;&gt; $mdfile echo \" lqip: data:image/jpeg;base64,$base64_string\" &gt;&gt; $mdfile\t\t echo \"---\" &gt;&gt; $mdfile\t\t\t((image_count++)) echo \"Image count is now $image_count\" &gt;&gt; /var/log/webp-converter/info.log\t else\t\techo \"Writing image link to $mdfile\" &gt;&gt; /var/log/webp-converter/info.log echo \"![$filename!]($image_path){: w=\\\"800\\\" h=\\\"600\\\" lqip=\\\"data:image/jpeg;base64,$base64_string\\\" }\" &gt;&gt; $mdfile fi\t\techo \"Finished processing $file\" &gt;&gt; /var/log/webp-converter/info.log [ -e \"$optimized\" ] &amp;&amp; rm $optimized\t echo \"$file processed successfully\" fi doneecho \"Script Finished.\" &gt;&gt; /var/log/webp-converter/info.logWith this script, you can upload the images to the uploads/import directory and the script monitors this directory to automatically create a new blog post, optimize the image, and generate the LQIP. Please note that you will need the correct login credentials for your remote server and the jpegoptim, cwebp and convert (part of ImageMagick) commands must be installed on your server for the script to work." }, { "title": "Erstellen von Einladungslinks zur Benutzerauthentifizierung in einer mobilen App", "url": "/posts/pyhton-implementieren-eines-einladungslinks/", "categories": "Programming, Python", "tags": "asynchronous programming, networking, token", "date": "2023-07-07 07:45:00 +0200", "snippet": "In diesem Tutorial gehen wir durch den Prozess der Erstellung von Einladungslinks, die es Benutzern ermöglichen, sich in Ihrer App zu authentifizieren und bestimmten Benutzergruppen beizutreten.Vor...", "content": "In diesem Tutorial gehen wir durch den Prozess der Erstellung von Einladungslinks, die es Benutzern ermöglichen, sich in Ihrer App zu authentifizieren und bestimmten Benutzergruppen beizutreten.Vorbereitung Sie benötigen eine mobile App, die mit Apple SignIn integriert ist. Ein Backend-Server, der in der Lage ist, JWT (JSON Web Token) zu generieren und zu validieren. In diesem Tutorial verwenden wir Quart, ein Python-Web-Framework. Ihre App muss eine Web-Domain haben, die Apple verifizieren kann (z.B. https://api0.example.com), und Sie müssen in der Lage sein, die apple-app-site-association (AASA) Datei dort bereitzustellen.Schritt 1: Erstellen der Einladungslink-Route auf dem Backend-ServerDie erste Aufgabe besteht darin, eine neue Route auf dem Backend-Server zu erstellen, die /get-invite-link genannt werden könnte. Diese Route generiert ein JWT, das Informationen über die Gruppen-ID enthält, zu der der Benutzer eingeladen wird, und sendet dieses Token zurück an die mobile App.@app.route('/get-invite-link', methods=['POST'])async def get_invite_link(): data = await request.get_json() group_id = data.get('group_id') if not group_id: return await make_response(jsonify({'error': 'Missing group_id'}), 400) # Create JWT payload payload = { 'group_id': group_id, 'exp': datetime.utcnow() + timedelta(hours=8) # 8 hours of validity } invite_token = jwt.encode(payload, JWT_SECRET, algorithm='HS256') # For the simplicity, let's assume your app's URL is https://app.example.com invite_link = f\"https://app.example.com/invite/{invite_token}\" return await make_response(jsonify({'invite_link': invite_link}), 200) Hinweis: In diesem Beispiel verwenden wir Quart, ein Python-Web-Framework, um die Route zu erstellen. Wenn Sie ein anderes Framework verwenden, müssen Sie die entsprechenden Methoden verwenden, um die Anfrage zu erhalten und die Antwort zu senden.Schritt 2: Erstellen des Einladungslinks in der mobilen AppIn der mobilen App erstellen Sie einen “Einladungslink erstellen” -Button, der eine Anfrage an die /get-invite-link-Route des Backend-Servers sendet und die Gruppen-ID übergibt. Der Server wird ein JWT generieren und zurück an die App senden. Die App wird dann die URL des Einladungslinks erstellen, indem sie das Token an die Basis-URL der App anhängt.Schritt 3: Teilen des EinladungslinksIn der App können Sie nun den Einladungslink über den gewünschten Kommunikationskanal teilen (z.B. E-Mail, Messaging-Apps, etc.).Schritt 4: Verwenden des EinladungslinksWenn der eingeladene Benutzer auf den Einladungslink klickt, wird die App geöffnet und die App sendet eine Anfrage an eine andere Route auf dem Server, z.B. /use-invite-link, und übergibt das Einladungs-Token. Der Server wird das Token validieren, die Benutzer-ID extrahieren und den Benutzer zur entsprechenden Gruppe hinzufügen.@app.route('/use-invite-link', methods=['POST'])async def use_invite_link(): data = await request.get_json() user_id = data.get('user_id') invite_token = data.get('invite_token') if not user_id or not invite_token: return await make_response(jsonify({'error': 'Missing user_id or invite_token'}), 400) try: payload = jwt.decode(invite_token, JWT_SECRET, algorithms=['HS256']) except jwt.ExpiredSignatureError: return await make_response(jsonify({'error': 'Invite token expired'}), 400) except jwt.InvalidTokenError: return await make_response(jsonify({'error': 'Invalid invite token'}), 400) group_id = payload.get('group_id') # Let's assume user_db_manager has a method to add user to a group user_db_manager = current_app.config['USERDB_MANAGER'] await user_db_manager.add_user_to_group(user_id, group_id) return await make_response(jsonDurch die Verwendung von JWT und Apple SignIn können Sie sicherstellen, dass nur authentifizierte Benutzer eingeladen werden und beitreten können. Beachten Sie, dass das Token eine begrenzte Gültigkeitsdauer hat (in unserem Beispiel 8 Stunden), um die Sicherheit zu erhöhen.Dieses Tutorial zeigt, wie Sie Einladungslinks in Ihrer App implementieren können, um Benutzern den Beitritt zu bestimmten Benutzergruppen zu ermöglichen. Mit einigen Anpassungen können Sie es auf die spezifischen Anforderungen Ihrer App abstimmen. Viel Spaß beim Programmieren!" }, { "title": "Improving Location Tracking in an iOS Application: Case Study of the Parkplatzmanager App", "url": "/posts/swift-improving-location-tracking/", "categories": "Programming, Swift", "tags": "ios, combine, networking, app optimization, location tracking, swiftui, case study, parkplatzmanager", "date": "2023-07-04 16:45:00 +0200", "snippet": "In today’s blog post, we’ll be exploring how we can leverage Apple’s SwiftUI and Combine frameworks to improve location tracking within our iOS apps. Our use case involves a parking management app,...", "content": "In today’s blog post, we’ll be exploring how we can leverage Apple’s SwiftUI and Combine frameworks to improve location tracking within our iOS apps. Our use case involves a parking management app, where we want to keep track of the user’s current location to provide accurate services.Initial StateInitially, our code had a lack of continuous location updates while the user was interacting with the app. This could potentially lead to inaccurate location data, which would affect the functionality of our parking management app.struct ParkingView: View { @EnvironmentObject var locationService: LocationService @State private var currentLocation: CLLocation? var body: some View { // Other views... .onAppear { os_log(\"Location updates started\", log: viewLog, type: .debug) locationService.startLocationUpdates() .sink { location in self.currentLocation = location os_log(\"Current location: %@\", log: viewLog, type: .debug, location.description) } } }}Continuous Location UpdatesTo make location tracking more accurate, we decided to implement continuous location updates.First, we made changes in LocationService by replacing the getCurrentLocation() method with startLocationUpdates(). This method would return a publisher emitting new locations as they were updated by the CLLocationManager.class LocationService: NSObject, ObservableObject { private let locationManager = CLLocationManager() private var locationSubject = PassthroughSubject&lt;CLLocation, Never&gt;() func startLocationUpdates() -&gt; AnyPublisher&lt;CLLocation, Never&gt; {\t locationManager.startUpdatingLocation()\t return locationSubject.eraseToAnyPublisher() }}Next, we modified the SwiftUI View’s .onAppear method to subscribe to this publisher, storing the returned AnyCancellable in a @State property. This is important because we need to manage the lifecycle of the subscription and make sure it gets cancelled when the view disappears.In the .onAppear method, we started the location updates and stored the new location updates in currentLocation:.onAppear { self.locationUpdateCancellable = self.locationService.startLocationUpdates() .sink { location in self.currentLocation = location }}To make sure that location updates stop when the view is no longer on screen, we added an .onDisappear method that cancels the subscription and stops location updates:.onDisappear { self.locationService.stopUpdatingLocation() self.locationUpdateCancellable?.cancel()}Enhanced Button ActionsWe also improved the action buttons within the app. Each button has a role-based action which uses the camera to scan VIN numbers and license plates, and then sends the updated status of the vehicle to the backend.Here is an anonymized example of an action button within the ParkingView:private func actionButton(title: String, iconName: String, statusText: String, action: @escaping () -&gt; Void) -&gt; some View { Button(action: { AVCaptureDevice.requestAccess(for: .video) { granted in if granted { action() self.vehicleStatus = statusText self.isShowingScannerModal = true } else { os_log(\"Camera access not granted\", log: errorLog, type: .error) } } }) { HStack { Image(systemName: \"qrcode\") Text(title) Image(systemName: iconName) } } .padding() .background(Color.blue) .foregroundColor(.white) .cornerRadius(10)}ConclusionWith these changes, our location tracking is now more accurate and efficient. The use of Combine allowed us to reactively update our app’s state based on the user’s location, and SwiftUI made it easy to manage the lifecycle of these updates. Additionally, the role-based actions provide a user-friendly way to update vehicle statuses within the parking management app.[YouTube Video Placeholder]In our next post, we’ll explore further improvements we can make to our parking management app using more advanced SwiftUI and Combine techniques. Stay tuned!tags: [SwiftUI, Combine, iOS, Location Services]" }, { "title": "Optimizing an iOS Application: Case Study of the Parkplatzmanager App", "url": "/posts/swift-optimizing-ios-app/", "categories": "Programming, Swift", "tags": "ios, networking, app optimization, location tracking, combine", "date": "2023-07-04 11:45:00 +0200", "snippet": "In the last couple of days, we’ve embarked on an exciting journey of improving and optimizing an iOS application, the parkplatzmanager app. In this blog post, we will walk you through some of the c...", "content": "In the last couple of days, we’ve embarked on an exciting journey of improving and optimizing an iOS application, the parkplatzmanager app. In this blog post, we will walk you through some of the changes we made and how they led to a more efficient and well-structured app.IntroductionThe parkplatzmanager app had been initially developed with a basic architecture, and as it grew in complexity, there were areas in the code that needed optimization and restructuring. Some of the issues we identified included ambiguous type inference, a lack of email validation, handling cache expiration, and dealing with data inconsistencies due to incorrect key-value mapping in the JSON response.Let’s walk through how we tackled each of these issues.1. Resolving Ambiguous Type InferenceSwift, like many other languages, performs type inference to determine the type of an expression when it’s not explicitly provided. However, sometimes the compiler might face difficulties in inferring the type of an expression, especially when multiple type possibilities exist. We encountered such an issue in the ParkingService.swift file:return Promise { seal in firstly { RealDataRepository.shared.fetchUserData(userId: userId, groupId: groupId) }.done { vehicleData in do { // Process the data seal.fulfill(()) } catch let error { seal.reject(error) } }.catch { error in seal.reject(error) }}Here, Swift was unable to infer the type of vehicleData. To resolve this, we explicitly provided the type of the vehicleData:RealDataRepository.shared.fetchUserData(userId: userId, groupId: groupId)}.done { (vehicleData: [Vehicle]) in do { // Process the data seal.fulfill(()) } catch let error { seal.reject(error) }2. Enforcing Email ValidationIn the InviteView.swift file, we observed that the app did not validate emails, potentially leading to incorrect or invalid email addresses being accepted.We first introduced a method isValidEmail(_ email: String) -&gt; Bool which was initially left as a placeholder. Later, we added a simple email validation logic:private func isValidEmail(_ email: String) -&gt; Bool { let emailRegex = \"[A-Z0-9a-z._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\" let emailTest = NSPredicate(format:\"SELF MATCHES %@\", emailRegex) return emailTest.evaluate(with: email)}Furthermore, we made the text field in SwiftUI ignore auto-capitalization, ensuring that only lower case characters are allowed:TextField(\"Email\", text: $email) .keyboardType(.emailAddress) .autocapitalization(.none)3. Handling Cache ExpirationOur app uses Redis for caching, and we identified a need to set a time-to-live (TTL) or expiration for our keys to control the freshness of the data. This can be done using the EXPIRE command in Redis:await redis.set(cache_key, json.dumps(vehicle))await redis.expire(cache_key, 300) # Set a TTL of 300 seconds4. Addressing JSON Key-Value Mapping InconsistenciesThe application experienced crashes due to mismatches between the expected and received JSON data, particularly a missing vehicle key. We realized that the received JSON’s structure was not what the app expected.To fix this, we updated our model to matchthe actual data received, ensuring that the keys in the Swift model matched the keys in the JSON data.struct ResponseData: Codable { let status: String let data: [Vehicle]}struct Vehicle: Codable { let _id: String let fullName: String? let groupId: String let latitude: Double let licensePlate: String let longitude: Double let timestamp: String let userId: String let vehicleStatus: String let vin: String? enum CodingKeys: String, CodingKey { case _id case fullName case groupId = \"group_id\" case latitude case licensePlate = \"license_plate\" case longitude case timestamp case userId = \"user_id\" case vehicleStatus = \"vehicle_status\" case vin }}ConclusionOptimizing an app often involves refactoring and improving the existing code to ensure it’s efficient, scalable, and maintainable. In our case, we were able to solve type inference ambiguity, improve data validation, handle cache expiration, and deal with JSON key-value inconsistencies. These changes significantly improved the parkplatzmanager app’s robustness and efficiency, and we hope this walkthrough was insightful and helpful!" }, { "title": "Grundlagen zur Nutzung von Github Teil 2", "url": "/posts/github-einfuehrung-teil2/", "categories": "Github", "tags": "git, commit, branch, merge, clone, push, pull", "date": "2023-07-03 17:00:00 +0200", "snippet": "Titel: “Grundlagen von GitHub: Von Repositories bis zu Commits und Branches”Ein weiterer wichtiger Aspekt von GitHub sind die “Branches” und “Merges”. Diese Konzepte sind essenziell für das Verstän...", "content": "Titel: “Grundlagen von GitHub: Von Repositories bis zu Commits und Branches”Ein weiterer wichtiger Aspekt von GitHub sind die “Branches” und “Merges”. Diese Konzepte sind essenziell für das Verständnis, wie die Entwicklung in Git organisiert wird.Branches in GitHubEin “Branch” ist eine unabhängige Linie der Entwicklung in Ihrem Projekt. Stellen Sie sich das wie einen parallelen Pfad vor, auf dem Sie arbeiten können, ohne die Hauptlinie der Entwicklung (üblicherweise “master” oder “main” genannt) zu beeinflussen. Jeder Branch hat seinen eigenen Verlauf von Commits und kann unabhängig von anderen Branches bearbeitet werden.Ein neuer Branch wird oft erstellt, wenn Sie an einer neuen Funktion oder einem Bugfix arbeiten. So können Sie Änderungen vornehmen und testen, ohne den Hauptcode zu beeinträchtigen. Der Befehl git branch zeigt Ihnen alle vorhandenen Branches an, und mit git branch &lt;branch-name&gt; können Sie einen neuen Branch erstellen.Merges in GitHubWenn Sie Ihre Arbeit in einem Branch abgeschlossen haben und diese Änderungen in den Hauptzweig einfließen lassen wollen, führen Sie einen “Merge” durch. Bei einem Merge werden die Änderungen aus einem Branch in einen anderen übertragen.Der Befehl git merge &lt;branch-name&gt; führt den angegebenen Branch in den aktuell aktiven Branch ein. Eventuelle Konflikte, die dabei auftreten (z.B. wenn in beiden Branches die gleiche Zeile in einer Datei geändert wurde), müssen manuell gelöst werden.FazitBranches und Merges sind zentrale Bestandteile der Arbeit mit Git und GitHub. Sie ermöglichen paralleles Arbeiten und erleichtern die Integration von Änderungen. Mit der Zeit werden Sie feststellen, dass die Nutzung von Branches und Merges den Arbeitsfluss erheblich erleichtern kann.Wir hoffen, dass dieser Blogbeitrag Ihnen dabei hilft, den vollen Nutzen aus GitHub zu ziehen. Bei weiteren Fragen oder Unklarheiten, zögern Sie nicht, einen Kommentar zu hinterlassen oder uns direkt zu kontaktieren. Viel Spaß beim Coden!" }, { "title": "Grundlagen zur Nutzung von Github Teil 1", "url": "/posts/github-einfuehrung-teil1/", "categories": "Github", "tags": "git, branch, commit, repository", "date": "2023-07-03 09:00:00 +0200", "snippet": "Die Nutzung von GitHub: Ein umfassender LeitfadenGitHub ist eine webbasierte Hosting-Plattform für die Versionsverwaltung Git. Es bietet eine Reihe von Funktionen, die das Arbeiten in Gruppen erlei...", "content": "Die Nutzung von GitHub: Ein umfassender LeitfadenGitHub ist eine webbasierte Hosting-Plattform für die Versionsverwaltung Git. Es bietet eine Reihe von Funktionen, die das Arbeiten in Gruppen erleichtern und zur gemeinsamen Entwicklung von Software-Projekten genutzt werden. In diesem Blog-Beitrag gehen wir tief in die Nutzung von GitHub ein und erklären Begriffe wie Repository, Commit, Pull, Push, Stash und vieles mehr.GitHub RepositoryEin Repository (oder “Repo”) ist ein Speicherort für ein Projekt. Es enthält alle Projektdateien und jede Änderung, die an diesen Dateien vorgenommen wird. Ein GitHub-Repository enthält nicht nur den Projektcode, sondern auch die Versionshistorie und weitere Informationen wie Issues, Projektboards und Aktionen.Erstellen Sie ein neues Repository, indem Sie auf der GitHub-Hauptseite auf die Schaltfläche “New” klicken. Geben Sie Ihrem Repository einen Namen und eine Beschreibung. Sie können es öffentlich machen, damit jeder es sehen und daran arbeiten kann, oder privat, damit nur Sie und die von Ihnen eingeladenen Personen darauf zugreifen können.Git CommitEin “Commit” ist eine Änderung, die Sie an den Dateien in Ihrem Repository vornehmen. Jeder Commit hat eine eindeutige ID, die Sie verwenden können, um auf spezifische Änderungen zu verweisen. Ein Commit enthält auch eine Commit-Nachricht, die beschreibt, was in diesem Commit geändert wurde.Um einen Commit zu erstellen, machen Sie zuerst Änderungen an den Dateien in Ihrem Repository. Wenn Sie fertig sind, können Sie mit dem Befehl git add die geänderten Dateien zur “Staging Area” hinzufügen. Danach können Sie mit git commit -m \"Ihre Nachricht\" einen neuen Commit mit Ihrer Änderung erstellen.Git PushMit dem Befehl “Push” laden Sie Ihre lokalen Änderungen in Ihr GitHub-Repository hoch. Der Befehl git push origin master beispielsweise lädt alle Änderungen, die Sie in Ihrem lokalen “master”-Branch gemacht haben, auf GitHub hoch.Git Pull“Pull” ist der umgekehrte Vorgang zu “Push”. Mit einem “Pull” holen Sie die neuesten Änderungen aus Ihrem GitHub-Repository und aktualisieren damit Ihren lokalen Code. Der Befehl dazu lautet git pull origin master, um die neuesten Änderungen vom “master”-Branch herunterzuladen.Git StashMit dem Befehl git stash können Sie Änderungen, die Sie noch nicht committen möchten, vorübergehend beiseitelegen. Diese Änderungen werden gespeichert und können später wieder hervorgeholt werden. Das ist besonders nützlich, wenn Sie mitten in der Arbeit an einer Funktion sind und schnell zu einem anderen Branch wechseln müssen.FazitDas Arbeiten mit GitHub kann zu Beginn kompliziert erscheinen, aber mit ein wenig Übung wird es schnell zu einem mächtigen Werkzeug für die Softwareentwicklung. Die Befehle und Konzepte, die wir in diesem Blogbeitrag besprochen haben, sind die Grundlage für die Arbeit mit GitHub. Sie ermöglichen es Ihnen, Ihre Projekte effektiv zu verwalten, zusammen mit anderen zu arbeiten undden Überblick über Ihre Änderungen zu behalten.Wir hoffen, dass dieser Beitrag Ihnen einen guten Überblick über die Grundlagen von GitHub gegeben hat. Viel Spaß beim Coden!" }, { "title": "Einrichtung und Fehlerbehebung eines Lychee-Bilderservers", "url": "/posts/linux-lychee-image-server-copy/", "categories": "Homelab, VMs", "tags": "nginx, webserver, lychee, php, mysql, postgresql, sqlite3, linux", "date": "2023-07-03 01:00:00 +0200", "snippet": "Lychee ist eine großartige Open-Source-Lösung zur Verwaltung und Organisation Ihrer Fotos. In diesem Beitrag zeige ich Ihnen, wie Sie einen Lychee-Bilderserver einrichten und potenzielle Probleme b...", "content": "Lychee ist eine großartige Open-Source-Lösung zur Verwaltung und Organisation Ihrer Fotos. In diesem Beitrag zeige ich Ihnen, wie Sie einen Lychee-Bilderserver einrichten und potenzielle Probleme beheben können.Schritt 1: SystemvoraussetzungenUm Lychee zu installieren, stellen Sie sicher, dass Ihr Server folgendes vorweist: Ein Webserver wie Apache oder nginx. Eine Datenbank: MySQL (version &gt; 5.7.8), MariaDB (version &gt; 10.2), PostgreSQL (version &gt; 9.2), oder SQLite3. PHP &gt;= 8.0 mit entsprechenden PHP-Erweiterungen. Imagick-Extension für bessere Thumbnail-Generierung.Schritt 2: Lychee herunterladen und installierenBeginnen wir mit dem Herunterladen von Lychee. Sie können Lychee direkt von GitHub klonen:git clone https://github.com/LycheeOrg/Lychee.gitWechseln Sie in das Verzeichnis und installieren Sie die erforderlichen Abhängigkeiten mit Composer:cd Lycheecomposer install --no-devSchritt 3: Datenbank konfigurierenErstellen Sie eine neue Datenbank für Lychee und behalten Sie die Zugangsdaten bei. Sie benötigen diese, wenn Sie Lychee zum ersten Mal starten.mysql -u root -pCREATE DATABASE lychee;CREATE USER 'lychee'@'localhost' IDENTIFIED BY 'password';GRANT ALL PRIVILEGES ON lychee.* TO 'lychee'@'localhost';FLUSH PRIVILEGES;exit;Schritt 4: Nginx-KonfigurationHier ist ein einfaches Beispiel für eine Nginx-Konfigurationsdatei:nginxserver { listen 80; server_name your-domain.com; root /path/to/your/lychee/public; index index.php; location / { try_files $uri $uri/ /index.php?$query_string; } location ~ \\.php$ { fastcgi_split_path_info ^(.+\\.php)(/.+)$; fastcgi_pass unix:/var/run/php/php8.0-fpm.sock; fastcgi_index index.php; include fastcgi_params; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; fastcgi_param PHP_VALUE \"post_max_size=100M upload_max_filesize=20M \"; }}Vergessen Sie nicht, Ihren Domainnamen und den Pfad zu Ihrem Lychee-Verzeichnis zu ändern. Nachdem Sie Ihre Konfigurationsdatei eingerichtet haben, vergewissern Sie sich, dass Nginx und PHP-FPM korrekt konfiguriert sind und laufen.Schritt 5: Berechtigungen setzenVergewissern Sie sich, dass die Berechtigungen für Ihr Lychee-Verzeichnis korrekt gesetzt sind. Sie können dies mit den folgenden Befehlen erreichen:sudo chown -R www-data:www-data /var/www/Lychee/sudo find /var/www/Lychee/ -type d -exec chmod 775 {} \\;sudo find /var/www/Lychee/ -type f -exec chmod 664 {} \\;Schritt 6: Lychee konfigurieren und nutzenÖffnen Sie nun Ihren Webbrowser und navigieren Sie zu Ihrer Lychee-Website. Sie werden aufgefordert, eine Datenbank zu erstellen und einen Benutzernamen und ein Passwort zu wählen.TroubleshootingManchmal könnten Sie auf einige Probleme stoßen. Hier sind einige gängige Lösungen:Fehler 502: Dieser Fehler tritt normalerweise auf, wenn Nginx versucht, mit dem PHP-FPM-Dienst zu kommunizieren und keine Antwort erhält. Überprüfen Sie, ob PHP-FPM läuft (systemctl status php8.1-fpm.service) und ob der Socket-Pfad in der Nginx-Konfigurationsdatei korrekt ist.Berechtigungsprobleme: Wenn Sie eine Fehlermeldung erhalten, dass bestimmte Verzeichnisse oder Dateien die falschen Berechtigungen haben, führen Sie die oben genannten chown und chmod Befehle erneut aus.Fehler beim Hochladen von Bildern: Wenn Sie Probleme beim Hochladen von Bildern haben, überprüfen Sie die PHP-Einstellungen in Ihrer Nginx-Konfigurationsdatei. Stellen Sie sicher, dass post_max_size und upload_max_filesize hoch genug eingestellt sind.FazitMit diesen Schritten sollten Sie in der Lage sein, Lychee auf Ihrem Server zu installieren und zu konfigurieren. Bitte beachten Sie, dass die genaue Vorgehensweise je nach Ihrer spezifischen Serverkonfiguration variieren kann." }, { "title": "Fehlersuche beim Einrichten einer Datenbank-Verbindung in Quart", "url": "/posts/python-Fehlersuche-beim-Einrichten-einer-DB-Verbindung-in-Quart/", "categories": "Programming, Python", "tags": "quart, api, development, flask, database, async, await", "date": "2023-07-02 00:00:00 +0200", "snippet": "Fehlersuche beim Einrichten einer Datenbank-Verbindung in QuartDie Fehlermeldung KeyError: 'USER_DB_MANAGER' in einer Quart-Anwendung kann auftreten, wenn versucht wird, auf ein Konfigurationseleme...", "content": "Fehlersuche beim Einrichten einer Datenbank-Verbindung in QuartDie Fehlermeldung KeyError: 'USER_DB_MANAGER' in einer Quart-Anwendung kann auftreten, wenn versucht wird, auf ein Konfigurationselement zuzugreifen, das noch nicht in die Anwendungskonfiguration aufgenommen wurde. Im folgenden Artikel werden wir die möglichen Ursachen dieses Problems untersuchen und Vorschläge zur Lösung dieses Problems geben.ProblemDie Fehlermeldung sieht so aus:KeyError: 'USER_DB_MANAGER'Dieser Fehler weist darauf hin, dass der Schlüssel ‘USER_DB_MANAGER’ im aktuellen app-Konfigurationsobjekt nicht gefunden wird. Dies deutet darauf hin, dass die Initialisierung und Speicherung der Manager in der App-Konfiguration in der Startup-Routine Ihrer Anwendung möglicherweise nicht erfolgreich durchgeführt wurde.LösungStellen Sie sicher, dass die Initialisierung der Manager (einschließlich USER_DB_MANAGER) und ihre Speicherung in der App-Konfiguration vor den ersten Anfragen erfolgen. Dies geschieht typischerweise in der Setup-Funktion Ihrer App, die bei Start ausgeführt wird. Stellen Sie auch sicher, dass keine Exceptions während dieser Initialisierung ausgelöst werden, die dazu führen könnten, dass der Code zur Speicherung der Manager in der App-Konfiguration nicht erreicht wird.Ein Beispiel könnte wie folgt aussehen:@app.before_servingasync def startup(): \"\"\" This function is called before the app starts. It initializes the app container and the database managers. \"\"\" # Your code to initialize managers... user_db_manager = ... # Store managers into the app config for easy access app.config['USER_DB_MANAGER'] = user_db_managerSie können später in Ihrer Route darauf zugreifen:@user_api_blueprint.before_app_first_requestasync def setup(): user_db_manager = current_app.config['USER_DB_MANAGER'] # the rest of your code...FazitBei der Arbeit mit Quart und anderen asynchronen Web-Frameworks ist es wichtig, die Initialisierung von Ressourcen wie Datenbank-Verbindungen sorgfältig zu steuern. Stellen Sie sicher, dass alle erforderlichen Ressourcen korrekt initialisiert und in der Anwendungskonfiguration gespeichert sind, bevor Sie versuchen, sie in Ihren Anforderungshandlern zu verwenden." }, { "title": "Handling Asynchronous Tasks in iOS with Combine", "url": "/posts/swift-working-with-combine/", "categories": "Programming, Swift", "tags": "combine, ios, asynchronous, networking, swiftui", "date": "2023-07-01 12:00:00 +0200", "snippet": "Working with Combine FrameworkIn modern iOS development, handling asynchronous tasks efficiently is crucial. While there are several ways to handle asynchronous tasks in Swift, including closures, ...", "content": "Working with Combine FrameworkIn modern iOS development, handling asynchronous tasks efficiently is crucial. While there are several ways to handle asynchronous tasks in Swift, including closures, delegates, and DispatchQueue, one powerful approach is to use the Combine framework introduced in iOS 13.The Combine framework allows you to work with asynchronous events as if they were sequences of values you can transform and manipulate using high-order functions like map, filter, reduce, etc.The example code we have been looking at makes extensive use of the Combine framework for network requests. Here’s a method that sends a network request and returns a publisher:func request&lt;T: Encodable, U: Decodable&gt;(endPoint: APIEndpoint, parameters: T?, authRequired: Bool = true) -&gt; AnyPublisher&lt;U, Error&gt; { guard let request = buildRequest(from: endPoint, with: parameters, authRequired: authRequired) else { return Fail(error: NetworkError.invalidURL) .eraseToAnyPublisher() } return session.dataTaskPublisher(for: request) .tryMap { data, response in // handle response here... return data } .decode(type: U.self, decoder: JSONDecoder()) .mapError { error in // handle error here... return NetworkError.unknownError(error) } .eraseToAnyPublisher()}In this method, a network request is initiated with session.dataTaskPublisher(for: request), which returns a publisher that emits the server’s response as a tuple of (Data, URLResponse).This is then transformed using tryMap to only keep the Data part and check the HTTP response status. The resulting data is then decoded into the expected response type U with .decode(type: U.self, decoder: JSONDecoder()).The mapError function maps any error that occurs during this process to our custom NetworkError type.The use of Combine here provides several benefits: Composability: The publisher returned by the request method can be further composed with other publishers to handle complex asynchronous workflows. Error handling: Errors are propagated along the publisher chain and can be caught and handled at any point. Code readability: The high-level, declarative syntax of Combine makes the code more readable and easier to understand compared to nested closures or delegate methods. Integration with Swift UI: Combine works seamlessly with Swift UI, allowing you to easily update your UI based on the results of network requests.map and tryMapThe map function is used to transform the output of a publisher. For example, if you have a publisher that emits integers, you could use map to transform them into strings:let intPublisher = Just(5)let stringPublisher = intPublisher.map { \"The number is \\($0)\" }The tryMap function is similar, but it can throw errors, allowing you to handle possible failures during the transformation process.filterThe filter function is used to emit only the values that satisfy a given predicate. For example, you could create a publisher that only emits even numbers like this:let numbers = [1, 2, 3, 4, 5, 6].publisherlet evenNumbers = numbers.filter { $0 % 2 == 0 }combineLatestThe combineLatest function is used when you need to combine the latest values of multiple publishers. It emits a value whenever any of its input publishers emit a value, combining the latest values from each one.let publisher1 = PassthroughSubject&lt;Int, Never&gt;()let publisher2 = PassthroughSubject&lt;String, Never&gt;()let combined = publisher1.combineLatest(publisher2)publisher1.send(1)publisher2.send(\"a\") // Emits (1, \"a\")publisher1.send(2) // Emits (2, \"a\")publisher2.send(\"b\") // Emits (2, \"b\")mergeThe merge function combines the outputs from multiple publishers into a single publisher. Unlike combineLatest, it does not wait for each publisher to emit a value, but emits values as soon as they arrive from any publisher.switchToLatestThe switchToLatest operator is used when you have a publisher of publishers and you want to transform it into a publisher that emits only the latest values from the latest publisher. This is particularly useful when working with asynchronous tasks like network requests.zipThe zip operator combines multiple publishers by pairing their values together. Unlike combineLatest, it emits a value only when all of its input publishers have emitted a value.These are just a few examples of the operators available in the Combine framework. By composing these operators together, you can express complex asynchronous workflows in a clear and concise way.However, as with any tool, it’s important to understand its strengths and limitations. While Combine is extremely powerful for handling asynchronous tasks, it also has a steep learning curve and requires a good understanding of Swift and functional programming concepts. For simpler tasks, other approaches like closures or the new async/await syntax introduced in Swift 5.5 might be more suitable.In the next part of this series, we will explore how to test Combine code and handle common pitfalls. Stay tuned!" }, { "title": "Building API Routes with Quart", "url": "/posts/pyhton-building-api-routes-with-quart/", "categories": "Programming, Python", "tags": "quart, api, development, python, flask, async, await", "date": "2023-06-30 19:00:00 +0200", "snippet": "Building API Routes with QuartIn this blog post, we’ll go over how to use the Quart library in Python to create API endpoints. Quart is a Python ASGI web microframework whose API is a superset of t...", "content": "Building API Routes with QuartIn this blog post, we’ll go over how to use the Quart library in Python to create API endpoints. Quart is a Python ASGI web microframework whose API is a superset of the Flask API, meaning that you can use Flask-like syntax and patterns with Quart.from quart import Quart, request, jsonify, make_responseapp = Quart(__name__)@app.route('/api/test', methods=['GET'])async def test_route(): return jsonify({\"message\": \"This is a test route\"}), 200app.run()User Registration and LoginHere are examples of routes for user registration and login, which handle HTTP POST requests and use the async/await syntax.@app.route('/user/register', methods=['POST'])async def register(): data = await request.get_json() username = data.get('username') password = data.get('password') # Registration logic goes here... return jsonify({\"message\": f\"User {username} registered successfully\"}), 200@app.route('/user/login', methods=['POST'])async def login(): data = await request.get_json() username = data.get('username') password = data.get('password') # Login logic goes here... return jsonify({\"message\": \"Login successful\"}), 200Error HandlingYou can also return error responses with appropriate HTTP status codes. Below is an example of error handling for a missing parameter.@app.route('/user/login', methods=['POST'])async def login(): data = await request.get_json() username = data.get('username') password = data.get('password') if not username or not password: return jsonify({\"error\": \"Missing username or password\"}), 400 # Rest of the login logic goes here... return jsonify({\"message\": \"Login successful\"}), 200JSON Web Tokens (JWT)Quart can be combined with other libraries such as PyJWT to handle JSON Web Tokens (JWT) for authentication. Here’s an example route that verifies an identity token.@app.route('/verifyIdentityToken', methods=['POST'])async def verify_identity_token(): data = await request.get_json() id_token = data.get('identity_token') # Token verification logic goes here... return await make_response(jsonify({'user_id': user_id, 'jwt_token': jwt_token}), 200)That’s it for this quick look at Quart. The library is a powerful tool for building API endpoints in Python, and it is especially useful when combined with other libraries to handle things like JWT for authentication." }, { "title": "Debugging Network Requests in iOS with Swift", "url": "/posts/swift-debugging-network-requests-in-ios/", "categories": "Programming, Swift", "tags": "combine, ios, asynchronous, networking, swiftui", "date": "2023-06-29 13:00:00 +0200", "snippet": "Debugging Network Requests in iOS with SwiftIn this article, we are going to discuss an essential part of every mobile application that interacts with a backend server - network requests. Specifica...", "content": "Debugging Network Requests in iOS with SwiftIn this article, we are going to discuss an essential part of every mobile application that interacts with a backend server - network requests. Specifically, we’ll focus on how to debug network requests in an iOS application using Swift.While developing an iOS application, it’s very common to encounter issues with network requests such as invalid request formats, issues with authorization or unexpected server responses. Being able to effectively debug these network requests can save you a lot of time and effort.Let’s start with the most straightforward way of debugging - logging.LoggingOne of the most common ways to debug network requests is to log essential parts of the requests and responses to the console. This includes the request’s URL, method, headers, and body, as well as the response’s status code and body.Swift’s os_log API can be used for this purpose. It provides a powerful, flexible way to efficiently log diagnostic messages from your app.Here is a simple example of how you can use os_log to log network request and response details:os_log(\"Starting request to endpoint: %@\", log: self.log, type: .info, endPoint.path)...os_log(\"Built request: %@\", log: self.log, type: .debug, request.debugDescription)os_log(\"Request httpBody: %@\", log: self.log, type: .debug, String(data: request.httpBody ?? Data(), encoding: .utf8) ?? \"Not decodable to string\")os_log(\"Request headers: %@\", log: self.log, type: .debug, request.allHTTPHeaderFields?.description ?? \"None\")os_log(\"Request URL: %@\", log: self.log, type: .debug, request.url?.absoluteString ?? \"None\")...os_log(\"Received data: %@\", log: self.log, type: .debug, String(data: data, encoding: .utf8) ?? \"Not decodable to string\")os_log(\"HTTP error: %d\", log: self.log, type: .error, httpResponse.statusCode)However, it’s important to be careful with what you log. Do not log sensitive data like passwords, tokens, or personally identifiable information (PII). This is particularly relevant when working with JWT tokens or similar, which often carry sensitive data.Anonymizing Sensitive DataFor instance, you might want to log the identity token used for signing in with Apple. However, this token contains sensitive information and should not be logged in its entirety.You could instead log a part of the token, for example the first few and last few characters, and replace the rest with placeholders:let anonymizedToken = String(identityToken.prefix(8)) + \"...\" + String(identityToken.suffix(8))os_log(\"Verifying identity token: %@\", log: self.log, type: .info, anonymizedToken)This will give you enough information to identify individual tokens in the logs, without revealing sensitive information.Checking the Server LogsWhile the client logs can give you a lot of information about what’s happening on the device, sometimes the problem lies on the server side. If you have access to the server logs, you should also check them for any errors or warnings.Here’s a simple example of what you might find in the server logs:2023-06-30 21:01:47,838 - quartapp - INFO - Request to /verifyIdentityToken...2023-06-30 21:01:47,838 - quartapp - DEBUG - Request data: None2023-06-30 21:01:47,838 - quartapp - ERROR - Missing request bodyIn this case, the server log shows that the request body was missing from the request. This gives you a clear indication of what went wrong and where to look in your client code to fix the issue.In conclusion, debugging network requests might seem daunting at first, but with the right tools and practices, it becomes a manageable task. Remember to use logging effectively and always be mindful of security and privacy considerations when handling sensitive data. Happy debugging!Note: The code snippets used in this article are based on Swift 5 and the standard Swift and Foundation libraries available as of Xcode 13. Some details might differ in newer or older versions of Swift and Xcode." }, { "title": "Building an Audit Trail with Quart, MongoDB, and Redis", "url": "/posts/python-audit-trail/", "categories": "Programming, Python", "tags": "auditTrail, webDevelopment, backend, microservices, quart, mongodb, redis", "date": "2023-06-28 15:00:00 +0200", "snippet": "Building an Audit Trail with Quart, MongoDB, and RedisIn this post, we will explore how to build an Audit Trail system using Quart for Python, MongoDB, and Redis. This will involve implementing Mon...", "content": "Building an Audit Trail with Quart, MongoDB, and RedisIn this post, we will explore how to build an Audit Trail system using Quart for Python, MongoDB, and Redis. This will involve implementing MongoDB for data storage, Quart for the web server and API, and Redis for caching data. We will create several API routes for managing audit entries in a MongoDB database.Introduction to the ModulesOur code is organized in several modules. Each module is responsible for handling specific aspects of the audit trail system.The AuditTrailDBManager ClassThe AuditTrailDBManager is defined in app/common/managers/audittraildbmanager.py. This class is responsible for managing the operations of the AuditTrailDB, which is a MongoDB database that we use for storing our audit trail entries. It provides methods for connecting to the MongoDB server, inserting documents, finding documents based on a query, updating documents, deleting documents, and saving and loading model data.class AuditTrailDBManager(IMongoDB, IDependency): \"\"\" AuditTrailDBManager is a class for managing the AuditTrailDB operations.\"\"\" ...The AuditTrailDBManager uses the motor library for making asynchronous MongoDB operations.The Quart API RoutesWe have defined our API routes in app/quart/routes/mongo_api_routes.py. We use the Quart library for Python to create these routes. Quart allows us to create API routes in a manner similar to Flask, but with support for Python’s asyncio library for asynchronous operations.Our API has several routes: /apiv3/audit_trail/check-port: Checks if the API is running. /apiv3/audit_trail/entries: Supports the POST method for creating a new audit entry and the GET method for retrieving audit entries. /apiv3/audit_trail/entries/&lt;entry_id&gt;: Supports the PUT method for updating an audit entry with a given ID, and the DELETE method for deleting an audit entry with a given ID.Here is a code snippet for one of the routes:@audit_trail_api_blueprint.route('/entries', methods=['POST'])@verify_jwt_tokenasync def create_audit_entry(): \"\"\" Erstellt einen neuen Audit-Eintrag.\"\"\" ...Each route function uses the @verify_jwt_token decorator, which verifies the JWT token for user authentication.The API routes interact with the MongoDB database through the AuditTrailDataController.The AuditTrailDataController ClassThe AuditTrailDataController is responsible for controlling the data of the audit trail. It uses the AuditTrailDBManager to interact with the MongoDB database.The AuditTrailDataController is initialized before the first request to the Quart application is made:@audit_trail_api_blueprint.before_app_first_requestasync def init_audit_trail_data_controller(): \"\"\" Initialisiert den AuditTrailDataController\"\"\" ...Redis for CachingTo reduce the load on the MongoDB server and speed up our application, we use Redis for caching the results of the GET requests. We store the results of each unique query in Redis. The next time the same query is made, we return the cached results from Redis instead of querying the MongoDB database again.Here is a code snippet for theGET request route that uses Redis:@audit_trail_api_blueprint.route('/entries', methods=['GET'])@verify_jwt_tokenasync def get_audit_entries(): ... # Erhalte Redis-Instanz redis = await get_redis() # Konvertiere query dict in str für den Gebrauch mit Redis query_str = json.dumps(query, sort_keys=True) cached_entries = await redis.get(query_str) if cached_entries is not None: ... else: entries = await audittrail_data_controller.get_audit_entries(query) ...In this code snippet, we get the Redis instance and then convert the query dictionary into a string to use with Redis. If the query results are already cached in Redis, we return those results. Otherwise, we get the audit entries from MongoDB and store the results in Redis for future requests.ConclusionWe have discussed how to build an audit trail system using Quart for Python, MongoDB, and Redis. The system provides API routes for managing audit entries stored in a MongoDB database and uses Redis for caching query results. The AuditTrailDBManager class manages the MongoDB operations, and the AuditTrailDataController controls the audit trail data. This system is a good example of how to use Quart, MongoDB, and Redis together to create a highly efficient and scalable web application.In future posts, we’ll further dive into the usage of Quart and other technologies to build highly scalable microservices. Stay tuned!Aufbau eines Audit-Trail mit Quart, MongoDB und RedisIn diesem Beitrag werden wir untersuchen, wie man ein Audit Trail System mit Quart für Python, MongoDB und Redis erstellt. Dabei implementieren wir MongoDB für die Datenspeicherung, Quart für den Webserver und die API sowie Redis zur Zwischenspeicherung von Daten. Wir werden mehrere API-Routen erstellen, um Audit-Einträge in einer MongoDB-Datenbank zu verwalten.Einführung in die ModuleUnser Code ist in mehrere Module organisiert. Jedes Modul ist für bestimmte Aspekte des Audit Trail Systems zuständig.Die Klasse “AuditTrailDBManager”Der “AuditTrailDBManager” ist in “app/common/managers/audittraildbmanager.py” definiert. Diese Klasse ist für die Verwaltung der Operationen der “AuditTrailDB” zuständig, einer MongoDB-Datenbank, in der wir unsere Audit Trail Einträge speichern. Sie bietet Methoden zum Verbinden mit dem MongoDB-Server, zum Einfügen von Dokumenten, zum Suchen von Dokumenten basierend auf einer Abfrage, zum Aktualisieren von Dokumenten, zum Löschen von Dokumenten sowie zum Speichern und Laden von Modelldaten.class AuditTrailDBManager(IMongoDB, IDependency): \"\"\" AuditTrailDBManager ist eine Klasse zur Verwaltung der AuditTrailDB-Operationen.\"\"\" ...Der “AuditTrailDBManager” verwendet die Motor-Bibliothek für asynchrone MongoDB-Operationen.Die Quart-API-RoutenWir haben unsere API-Routen in “app/quart/routes/mongo_api_routes.py” definiert. Wir verwenden die Quart-Bibliothek für Python, um diese Routen zu erstellen. Quart ermöglicht es uns, API-Routen ähnlich wie Flask zu erstellen, unterstützt jedoch die asynchrone Funktionalität von Pythons asyncio-Bibliothek.Unsere API hat mehrere Routen: /apiv3/audit_trail/check-port: Überprüft, ob die API läuft. /apiv3/audit_trail/entries: Unterstützt die POST-Methode zum Erstellen eines neuen Audit-Eintrags und die GET-Methode zum Abrufen von Audit-Einträgen. /apiv3/audit_trail/entries/&lt;entry_id&gt;: Unterstützt die PUT-Methode zum Aktualisieren eines Audit-Eintrags mit einer bestimmten ID und die DELETE-Methode zum Löschen eines Audit-Eintrags mit einer bestimmten ID.Hier ist ein Code-Schnipsel für eine der Routen:@audit_trail_api_blueprint.route('/entries', methods=['POST'])@verify_jwt_tokenasync def create_audit_entry(): \"\"\" Erstellt einen neuen Audit-Eintrag.\"\"\" ...Jede Routenfunktion verwendet den Dekorator “@verify_jwt_token”, der das JWT-Token für die Benutzerauthentifizierung überprüft.Die API-Routen interagieren über den “AuditTrailDataController” mit der MongoDB-Datenbank.Die Klasse “AuditTrailDataController”Der “AuditTrailDataController” ist für die Steuerung der Daten des Audit Trails zuständig. Er verwendet den “AuditTrailDBManager”, um mit der MongoDB-Datenbank zu interagieren.Der “AuditTrailDataController” wird initialisiert, bevor die erste Anfrage an die Quart-Anwendung gestellt wird:@audit_trail_api_blueprint.before_app_first_requestasync def init_audit_trail_data_controller(): \"\"\" Initialisiert den AuditTrailDataController.\"\"\" ...Redis zur ZwischenspeicherungUm die Belastung des MongoDB-Servers zu reduzieren und unsere Anwendung zu beschleunigen, verwenden wir Redis zur Zwischenspeicherung der Ergebnisse der GET-Anfragen. Wir speichern die Ergebnisse jeder eindeutigen Abfrage in Redis. Beim nächsten Mal, wenn dieselbe Abfrage gestellt wird, geben wir die zwischengespeicherten Ergebnisse aus Redis zurück, anstatt die MongoDB-Datenbank erneut abzufragen.Hier ist ein Code-Schnipsel für die GET-Anfrage-Route, die Redis verwendet:@audit_trail_api_blueprint.route('/entries', methods=['GET'])@verify_jwt_tokenasync def get_audit_entries(): ... # Hole Redis-Instanz redis = await get_redis() # Konvertiere query dict in str für die Verwendung mit Redis query_str = json.dumps(query, sort_keys=True) cached_entries = await redis.get(query_str) if cached_entries is not None: ... else: entries = await audittrail_data_controller.get_audit_entries(query) ...In diesem Code-Schnipsel holen wir die Redis-Instanz und konvertieren dann das Abfrage-Dictionary in einen String, um es mit Redis zu verwenden. Wenn die Abfrageergebnisse bereits in Redis zwischengespeichert sind, geben wir diese Ergebnisse zurück. Andernfalls holen wir die Audit-Einträge aus MongoDB und speichern die Ergebnisse in Redis für zukünftige Anfragen.FazitWir haben besprochen, wie man ein Audit Trail System mit Quart für Python, MongoDB und Redis erstellt. Das System bietet API-Routen zur Verwaltung von Audit-Einträgen, die in einer MongoDB-Datenbank gespeichert sind, und verwendet Redis zur Zwischenspeicherung von Abfrageergebnissen. Die Klasse “AuditTrailDBManager” verwaltet die MongoDB-Operationen und der “AuditTrailDataController” steuert die Audit Trail Daten. Dieses System ist ein gutes Beispiel dafür, wie man Quart, MongoDB und Redis zusammen verwendet, um eine hoch effiziente und skalierbare Webanwendung zu erstellen.In zukünftigen Beiträgen werden wir noch weiter auf den Einsatz von Quart und anderen Technologien zur Entwicklung hoch skalierbarer Microservices eingehen. Bleiben Sie dran!" }, { "title": "Pyhton Quart Hypercorn", "url": "/posts/pyhton-quart-hypercorn/", "categories": "", "tags": "", "date": "2023-06-27 00:00:00 +0200", "snippet": "Erstellen einer Webanwendung mit Quart und Hypercorn in PythonIm heutigen Blogpost möchten wir uns auf das Erstellen von Webanwendungen mit Quart und Hypercorn in Python konzentrieren. Quart ist ei...", "content": "Erstellen einer Webanwendung mit Quart und Hypercorn in PythonIm heutigen Blogpost möchten wir uns auf das Erstellen von Webanwendungen mit Quart und Hypercorn in Python konzentrieren. Quart ist ein leistungsfähiges, asynchrones Web-Framework für Python, während Hypercorn ein ASGI-Server ist, der dazu dient, Ihre Quart-Anwendung zu hosten.Die Quart-Anwendung, die wir in diesem Beitrag erstellen, integriert mehrere Technologien und Konzepte, darunter Redis, RabbitMQ und PostgreSQL. Dieser Beitrag geht davon aus, dass Sie mit Python und Grundlagen der Webentwicklung vertraut sind.Die Hauptanwendung erstellenZuerst importieren wir die benötigten Module. Quart ist das Hauptmodul, das wir verwenden werden, um unsere Webanwendung zu erstellen. Wir importieren auch mehrere Blueprints, die unsere Anwendungsrouten definieren:# quartapp.pyimport asynciofrom quart import Quart from hypercorn.asyncio import serve from hypercorn.config import Config from quart_redis import RedisHandler from app.common.config import Configfrom app.common.appcontainer import AppContainerfrom app.quart.routes import (metrics_api_blueprint, data_api_blueprint, rabbitmq_api_blueprint, cors_blueprint, user_api_blueprint, group_api_blueprint, audit_trail_api_blueprint)In diesem Codeblock sehen Sie, dass wir verschiedene Blueprints importieren, die die verschiedenen Teile unserer Anwendung definieren. Ein Blueprint in Quart ist ein Modul, das mehrere Routen, Vorlagen und statische Dateien gruppiert. Mit Blueprints können Sie Ihre Anwendung in logische Abschnitte unterteilen, die getrennt voneinander entwickelt und getestet werden können.Registrieren von BlueprintsMit der register_blueprint-Methode von Quart registrieren wir jeden Blueprint:for bp in blueprints: app.register_blueprint(bp['blueprint'])Verbindung zur DatenbankWir erstellen eine Hilfsfunktion, um die Verbindung zur Datenbank herzustellen. Diese Funktion versucht mehrmals, eine Verbindung herzustellen und wartet zwischen den Versuchen, um dem Datenbankserver Zeit zu geben, verfügbar zu werden:async def connect_with_retry(connect_func, db_name, max_attempts=6, retry_interval=10): \"\"\" Try to connect to a database with a given connect_func.\"\"\" for attempt in range(max_attempts): try: await connect_func() logger.info(\"Connected successfully to %s !\", db_name) return except Exception as e: logger.info(\"Retrying in %s seconds... (%s/%s)\", retry_interval, attempt+1, max_attempts) await asyncio.sleep(retry_interval) logger.error(\"Max retry attempts reached. Could not connect to %s .\", db_name) raise SystemExit(f\"Could not connect to {db_name}.\")Starten und Beenden der AnwendungWir definieren startup und cleanup Funktionen, die Quart vor bzw. nach dem Start der Anwendung aufruft. In der startup Funktion verbinden wir uns mit den Datenbanken und initialisieren die Managerklassen, während wir in der cleanup Funktion die Verbindungen und Ressourcen aufräumen:@app.before_servingasync def startup(): ...@app.after_servingasync def cleanup(): ...Die Anwendung startenSchließlich erstellen wir die main-Funktion, um die Anwendung zu starten:if __name__ == \"__main__\": quart_config = create_quart_config() config = Config() config.bind = [f\"{quart_config.host}:{quart_config.port}\"] config.certfile = quart_config.certfile config.keyfile = quart_config.keyfile config.protocol = quart_config.protocol config.application_path = \"quartapp:app\" asyncio.run(serve(app, config))Dieser Code startet den Hypercorn-Server und bindet ihn an die von der Konfiguration angegebene Adresse und den Port. Danach serviert Hypercorn die Quart-Anwendung und wartet auf eingehende Anfragen.Das ist alles! Mit diesen wenigen Schritten haben wir eine grundlegende Quart-Anwendung erstellt und mit Hypercorn gehostet. In zukünftigen Blogposts werden wir uns eingehender mit den einzelnen Aspekten dieser Anwendung befassen und uns ansehen, wie wir diese Anwendung weiter anpassen und erweitern können.Schlüsselwörter: #Quart #Hypercorn #WebApp #Python #ASGI #APIs #WebEntwicklung" }, { "title": "Pi-Hole mit Unbound / IPv4 und IPv6", "url": "/posts/pihole-unbound-ipv6/", "categories": "Homelab", "tags": "pihole, dns", "date": "2023-02-10 22:00:00 +0100", "snippet": "Pi-Hole Adblocker in Docker mit Unbound und IPv4 / IPv6Mein SetupBei mir läuft der primäre DNS-Server auf einem Raspberry Pi 4 in einem Docker Container zusammen mit Unbound. Da ich den Pi-Hole auc...", "content": "Pi-Hole Adblocker in Docker mit Unbound und IPv4 / IPv6Mein SetupBei mir läuft der primäre DNS-Server auf einem Raspberry Pi 4 in einem Docker Container zusammen mit Unbound. Da ich den Pi-Hole auch für die lokale Namesauflösung verwende (*.local.cstrube.de) und mir Traeffik hierfür über Let’s Encrypt SSL Zerifikate erstellt brauche ich die Möglichkeit zusammen IPv4 und IPv6 auflösen zu können.1. Statische IP AdresseDem Raspberry Pi mit netplan eine statische IPv4 einrichten:sudo ip achristian@rpi1:~/pihole-unbound$ sudo ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether **:**:**:**:**:** brd ff:ff:ff:ff:ff:ff inet 10.0.50.10/24 brd 10.0.50.255 scope global eth0 valid_lft forever preferred_lft forever inet6 2a02:8071:50d1:2465:e65f:1ff:fe46:efae/64 scope global dynamic mngtmpaddr noprefixroute valid_lft 86345sec preferred_lft 14345sec inet6 fe80::****:***:****:****/64 scope link valid_lft forever preferred_lft foreverunter 2: eth0: sieht man hier den Netzwerknamen. Anschließend erstellen wir eine Datei die wir 01-netcfg.yaml nenen im /etc/netplan Verzeichniss.sudo nano /etc/netplan/01-netcfg.yaml network: ethernets: eth0: addresses: - 10.0.50.10/24 dhcp4: false gateway4: 10.0.50.1 nameservers: addresses: - 1.1.1.1 - 1.0.0.1 search: - workgroup version: 2Hier habe ich DHCP deaktiviert und als DNS Server Cloudflares DNS Server gesetzt (1.1.1.1 und 1.0.0.1)Als nächstes kommen folgende zwei Befehle:sudo netplan generatesudo netplan apply2. Port 53 freimachenDa ich auf dem Raspberry Pi ein Ubuntu 22.04 laufen lasse musste ich als nächstes den Port 53 freiräumen. Dieser wird standardmäßig von resolved belegt.Dafür editiert man die /etc/systemd/resolved.conf Datei:sudo nano /etc/systemd/resolved.confHier wird unter #DNSStubListener=yes die # weggenommen und von =yes auf =no geändert. Siehe Beispiel:[Resolve]# Some examples of DNS servers which may be used for DNS= and FallbackDNS=:# Cloudflare: 1.1.1.1#cloudflare-dns.com 1.0.0.1#cloudflare-dns.com 2606:4700:4700::1111#cloudflare-dns.com 260&gt;# Google: 8.8.8.8#dns.google 8.8.4.4#dns.google 2001:4860:4860::8888#dns.google 2001:4860:4860::8844#dns.go&gt;# Quad9: 9.9.9.9#dns.quad9.net 149.112.112.112#dns.quad9.net 2620:fe::fe#dns.quad9.net 2620:fe::9#dns.quad&gt;#DNS=#FallbackDNS=#Domains=#DNSSEC=no#DNSOverTLS=no#MulticastDNS=no#LLMNR=no#Cache=no-negative#CacheFromLocalhost=noDNSStubListener=no#DNSStubListenerExtra=#ReadEtcHosts=yes#ResolveUnicastSingleLabel=noNach einem Neustart ist Port 53 frei für unseren Docker Container3. Docker installierenAls erstes die Abhängigkeiten installieren:sudo apt-get install apt-transport-https ca-certificates software-properties-common -yAnschließend Docker mit dem offiziellen Script installierencurl -fsSL get.docker.com -o get-docker.sh &amp;&amp; sh get-docker.shJetzt fügen wir noch unseren User der Docker Gruppe hinzu, damit können wir Docker ohne sudo ausführen:sudo usermod -aG docker pi4. Docker für IPv6 vorbereiten:Out of the Box unterstützt Docker kein IPv6, deswegen müssen wir eine /etc/docker/daemon.json Datei erstellen:{ \"ipv6\": true, \"fixed-cidr-v6\": \"fe80::/64\"} Laut offizieller Dokumentation braucht man das nicht, allerdings hatte ich ohne die Datei Probleme damitEinmal Docker Daemon neustarten:sudo systemctl restart dockerAls nächstes brauchen wir iptables damit das Docker Netzwerk IPv6 Traffic empfängt:sudo ip6tables -t nat -A POSTROUTING -s fe80::/64 ! -o docker0 -j MASQUERADEund damit die Änderungen nach einem Neustart nicht verloren gehen gibts das hier:sudo apt install iptables-persistent netfilter-persistentEinfach mit “yes” bestätigen5. Setup Pi-Hole ContainerDa ich ein Fan von Docker Compose bin habe ich hier als erstes eine .env Datei mit den Variablen erstellt:FTLCONF_LOCAL_IPV4=10.0.50.10TZ=Europe/BerlinWEBPASSWORD=sehrsicherespasswortREV_SERVER=trueREV_SERVER_DOMAIN=local.cstrube.deREV_SERVER_TARGET=10.0.10.1REV_SERVER_CIDR=10.0.0.0/16HOSTNAME=pihole-unboundDOMAIN_NAME=pihole-unbound.local.cstrube.dePIHOLE_WEBPORT=80WEBTHEME=default-darkDie Datei liegt im gleichen Verzeichniss wie die docker-compose.yml:version: '3.0'volumes: etc_pihole-unbound:services: pihole: container_name: pihole-unbound image: cbcrowe/pihole-unbound:latest hostname: ${HOSTNAME} domainname: ${DOMAIN_NAME} ports: - 443:443/tcp - 53:53/tcp - 53:53/udp - ${PIHOLE_WEBPORT:-80}:80/tcp environment: - FTLCONF_LOCAL_IPV4=${FTLCONF_LOCAL_IPV4} - TZ=${TZ:-UTC} - WEBPASSWORD=${WEBPASSWORD} - WEBTHEME=${WEBTHEME:-default-dark} - REV_SERVER=${REV_SERVER:-false} - REV_SERVER_TARGET=${REV_SERVER_TARGET} - REV_SERVER_DOMAIN=${REV_SERVER_DOMAIN} - REV_SERVER_CIDR=${REV_SERVER_CIDR} - PIHOLE_DNS_=127.0.0.1#5335 - DNSSEC=\"true\" - DNSMASQ_LISTENING=single - ServerIPv6=\"fe80::***:****:****:****\" volumes: - etc_pihole-unbound:/etc/pihole:rw - ./etc_pihole_dnsmasq-unbound:/etc/dnsmasq.d:rw restart: unless-stopped Wichtig hierbei: ServerIPv6=”…” mit der eigenen ausgelesenen Link-Local Adresse aus “ip a” ausfüllen!/pihole-unbound/docker-compose.ymlDen Container starten wir dann mit:docker compose up -d --force-recreateJetzt muss nur noch im Router der DNS Server auf die IPv4 und IPv6 Adresse des Raspberry Pi geändert werden." }, { "title": "bind9 DNS Server", "url": "/posts/linux-bind-dns-server/", "categories": "Homelab, VMs", "tags": "servers, dns, linux", "date": "2023-02-07 22:00:00 +0100", "snippet": "Ein eigener DNS-Server mit BIND9 bietet folgende Vorteile: Kontrolle über die Namensauflösung: Ein eigener DNS-Server gibt Ihnen die Kontrolle über die Namensauflösung und ermöglicht es Ihnen, Ihr...", "content": "Ein eigener DNS-Server mit BIND9 bietet folgende Vorteile: Kontrolle über die Namensauflösung: Ein eigener DNS-Server gibt Ihnen die Kontrolle über die Namensauflösung und ermöglicht es Ihnen, Ihre eigenen Domänen und IP-Adressen zu verwalten. Verringerung der Latenzzeiten: Ein lokaler DNS-Server kann schnellere Antwortzeiten bieten, da er nicht auf externe DNS-Server angewiesen ist, um Anfragen zu beantworten. Höhere Verfügbarkeit: Ein eigener DNS-Server kann eine höhere Verfügbarkeit gewährleisten, da er nicht auf externe Server angewiesen ist und ein Ausfall dieser Server die Namensauflösung nicht beeinträchtigt. Schutz vor Angriffen: Ein eigener DNS-Server kann dazu beitragen, das Netzwerk vor DNS-basierten Angriffen zu schützen, indem er für eine sichere Übertragung der Daten sorgt. Flexibilität: BIND9 ist ein leistungsstarkes und flexibles System, das anpassbar ist, um spezifische Anforderungen und Bedürfnisse zu erfüllen.Umsetzung:Basis ist ein Ubuntu 22.04 LXC in meinem Proxmox Cluster, vorbereitet mit Docker Engine und Docker ComposeOrdnerstruktur:/bind9/docker-compose.yml/bind9/records//bind9/cache/ docker-compose.ymlversion: \"3\"services: bind9: container_name: dnsserver image: ubuntu/bind9:latest environment: - BIND9_USER=root - TZ=Europe/Berlin ports: - \"53:53/tcp\" - \"53:53/udp\" volumes: - ./config:/etc/bind - ./cache:/var/cache/bind - ./records:/var/lib/bind restart: unless-stopped/bind9/config/named.conf Achtung, hier die eigenen IP Adressen eintragenacl internal { 10.0.10.0/24; 10.0.20.0/24; 10.0.30.0/24; 10.0.70.0/24;};options { forwarders { 1.1.1.1; 1.0.0.1; }; allow-query { internal; };};zone \"local.cstrube.de\" IN { type master; file \"/etc/bind/local.cstrube.zone\";};/bind9/config/local.cstrube.zone local.cstrube.zone$TTL 2d$ORIGIN local.cstrube.de.@ IN SOA ns.local.cstrube.de. info.cstrube.de. ( 2023070222 ; serial 12h ; refresh 15m ; retry 3w ; expire 2h ; minimum ttl ) IN NS ns.local.cstrube.de.ns IN A 10.0.20.11; -- add dns records belowpvecm2 IN A 10.0.10.72pvecm3 IN A 10.0.10.79" }, { "title": "Ubuntu VM mit GPU Passthrough", "url": "/posts/neuer-ubuntu-desktop/", "categories": "Homelab, VMs", "tags": "gpu, ubuntu", "date": "2023-01-29 10:00:00 +0100", "snippet": "Linux Ubuntu 22.04 LTS VM mit GPU NVIDIA Georce 1050Ti UnterstützungEine virtuelle Maschine mit GPU-Beschleunigung auf einem Server bietet folgende Vorteile: Erhöhte Leistung: GPU-Beschleunigu...", "content": "Linux Ubuntu 22.04 LTS VM mit GPU NVIDIA Georce 1050Ti UnterstützungEine virtuelle Maschine mit GPU-Beschleunigung auf einem Server bietet folgende Vorteile: Erhöhte Leistung: GPU-Beschleunigung kann eine signifikante Leistungssteigerung für bestimmte Anwendungen bieten, insbesondere bei grafikintensiven Aufgaben wie Video- und Bildbearbeitung, künstlicher Intelligenz und maschinellem Lernen. Flexibilität: Durch den Einsatz von virtuellen Maschinen kann ein einziger Server mehrere verschiedene Workloads ausführen, die jeweils GPU-Beschleunigung benötigen, wodurch Ressourcen effizienter genutzt werden. Skalierbarkeit: Da virtuelle Maschinen leicht hinzugefügt oder entfernt werden können, kann die GPU-Beschleunigungskapazität des Servers einfach skaliert werden, um den Anforderungen zu entsprechen. Kosteneffizienz: Ein Server mit GPU-Beschleunigung kann kosteneffizienter sein als die Bereitstellung einzelner Workstations mit dedizierten GPUs, insbesondere für kleinere Organisationen oder Unternehmen mit begrenztem Budget. Inhaltsverzeichnis: installiere Proxmox auf Server Setup IOMMU neue Ubuntu VM hinzufügen config Ubuntu VM with PCIE Passthrough1. Proxmox installieren:Am besten der Anleitung hier folgen.2. Setup IOMMU (I/O Memory Management Unit)Hier ist die Anleitung um das Durchreichen vom PCIE Komponenten zu ermöglichen.Das Ergebniss sollte dann so in etwa aussehen:root@pvecm2:~# lspci00:00.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir Root Complex00:00.2 IOMMU: Advanced Micro Devices, Inc. [AMD] Renoir IOMMU00:01.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe Dummy Host Bridge00:01.1 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe GPP Bridge00:02.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe Dummy Host Bridge00:02.1 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe GPP Bridge00:02.2 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe GPP Bridge00:08.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Renoir PCIe Dummy Host Bridge00:08.1 PCI bridge: Advanced Micro Devices, Inc. [AMD] Renoir Internal PCIe GPP Bridge to Bus00:14.0 SMBus: Advanced Micro Devices, Inc. [AMD] FCH SMBus Controller (rev 51)00:14.3 ISA bridge: Advanced Micro Devices, Inc. [AMD] FCH LPC Bridge (rev 51)00:18.0 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166a00:18.1 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166b00:18.2 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166c00:18.3 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166d00:18.4 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166e00:18.5 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 166f00:18.6 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 167000:18.7 Host bridge: Advanced Micro Devices, Inc. [AMD] Device 167101:00.0 VGA compatible controller: NVIDIA Corporation GP107 [GeForce GTX 1050 Ti] (rev a1)01:00.1 Audio device: NVIDIA Corporation GP107GL High Definition Audio Controller (rev a1)02:00.0 USB controller: Advanced Micro Devices, Inc. [AMD] Device 43ee02:00.1 SATA controller: Advanced Micro Devices, Inc. [AMD] Device 43eb02:00.2 PCI bridge: Advanced Micro Devices, Inc. [AMD] Device 43e903:00.0 PCI bridge: Advanced Micro Devices, Inc. [AMD] Device 43ea04:00.0 Ethernet controller: Realtek Semiconductor Co., Ltd. RTL8125 2.5GbE Controller (rev 04)05:00.0 Non-Volatile memory controller: Sandisk Corp WD Black SN850 (rev 01)06:00.0 Non-Essential Instrumentation [1300]: Advanced Micro Devices, Inc. [AMD] Zeppelin/Raven/Raven2 PCIe Dummy Function (rev c9)06:00.1 Audio device: Advanced Micro Devices, Inc. [AMD/ATI] Device 163706:00.2 Encryption controller: Advanced Micro Devices, Inc. [AMD] Family 17h (Models 10h-1fh) Platform Security Processor06:00.3 USB controller: Advanced Micro Devices, Inc. [AMD] Renoir USB 3.106:00.4 USB controller: Advanced Micro Devices, Inc. [AMD] Renoir USB 3.106:00.6 Audio device: Advanced Micro Devices, Inc. [AMD] Family 17h (Models 10h-1fh) HD Audio Controller3. neue VM erstellen: Öffne die Proxmox-Web-Oberfläche Klick auf den Reiter “VMs” Wähle “Create VM” im Dropdown-Menü oder klicke auf den grünen “Create VM”-Button Wähle den gewünschten Betriebssystemtyp aus und gib die grundlegenden Einstellungen wie RAM und CPU-Kerne an (Maschinentyp Q35, UEFI) Wähle ein Storage-Ziel für die neue VM aus Lade ein Betriebssystem-Image hoch oder wähle ein bereits vorhandenes Image Klicke auf “Create”4. VM für Passthrough konfigurieren:Achtung: Die VM ist nach dem hinzufügen der GPU nicht mehr über die Console erreichbar. Vorher sicherstellen das man mit VNC oder RDP auf die Maschine zugreifen kann. Ubuntu 22.04 hat z.B. RDP mit an Bord. Gehe zur Proxmox-Web-Oberfläche und öffne den virtuellen Maschinen-Editor Wähle im Reiter “Hardware” den PCIE-Gerätetyp aus und klicke auf “PCI passthrough” Markiere das zu übertragende Gerät und klicke auf “Add” Setze die Häckchen bei: (X)All Funktions, (X) Primary GPU, (X) ROM BAR, (X) PCI-Express Starte die virtuelle Maschine neu Überprüfe im Betriebssystem der virtuellen Maschine, ob das Gerät erkannt wurde5. NVIDIA X-Window Treiber installieren" }, { "title": "openai ChatGPT", "url": "/posts/ChatGPT/", "categories": "AI", "tags": "dell, hp, supermicro", "date": "2022-12-18 01:00:01 +0100", "snippet": "ChatGPT", "content": "ChatGPT" }, { "title": "Konfiguration von Jekyll als systemd-Dienst auf Ubuntu", "url": "/posts/linux-jekyll-systemd-ubuntu/", "categories": "Homelab", "tags": "pi, mdns, nginx, webserver, raspberry, ubuntu, systemd, jekyll", "date": "2022-08-01 14:00:00 +0200", "snippet": "EinleitungIn diesem Beitrag erfahren Sie, wie Sie Jekyll als systemd-Dienst auf einem Ubuntu-Server einrichten. Dies ist besonders nützlich, um sicherzustellen, dass Ihre Jekyll-Website immer läuft...", "content": "EinleitungIn diesem Beitrag erfahren Sie, wie Sie Jekyll als systemd-Dienst auf einem Ubuntu-Server einrichten. Dies ist besonders nützlich, um sicherzustellen, dass Ihre Jekyll-Website immer läuft, selbst nach einem Neustart des Servers.VoraussetzungenBevor Sie beginnen, stellen Sie sicher, dass Sie folgende Voraussetzungen erfüllen: Einen Ubuntu-Server, eine VM oder einen LXC-Container Ruby und Bundler installiert (siehe Anleitung) Ein Jekyll-Projekt, beispielsweise basierend auf dieser Vorlage: jekyll-docs-site @timothystewart6KonfigurationSchritt 1: Umgebungsvariable festlegenÖffnen Sie die .bashrc oder .bash_profile Datei in Ihrem Home-Verzeichnis und fügen Sie die folgende Zeile hinzu:export JEKYLL_ENV=productionSchritt 2: Konfiguration ladenFühren Sie den folgenden Befehl aus, um die neue Konfiguration zu laden:source ~/.bashrcSchritt 3: Gems installierenNavigieren Sie in Ihr Jekyll-Projektverzeichnis und führen Sie die folgenden Befehle aus, um alle notwendigen Gems mit Bundler zu installieren:bundle config set --local path 'vendor/bundle'bundle installDies sorgt dafür, dass alle Gems in einem Unterordner (vendor/bundle) Ihres Projekts installiert werden.Schritt 4: systemd-Service-Datei erstellenErstellen Sie eine systemd-Service-Datei für Jekyll. Erstellen Sie die Datei /etc/systemd/system/jekyll.service und fügen Sie den folgenden Inhalt ein:[Unit]Description=Jekyll BuildAfter=network.target[Service]Type=simpleUser=christianWorkingDirectory=/home/IHR_BENUTZERNAME/GITHUB_USERNAME.github.ioEnvironment=\"BUNDLE_PATH=/home/IHR_BENUTZERNAME/GITHUB_USERNAME.github.io/vendor/bundle\"Environment=\"GEM_HOME=/home/IHR_BENUTZERNAME/.gem/ruby/3.0.0\"Environment=\"JEKYLL_ENV=production\"ExecStart=/bin/bash -lc '/home/IHR_BENUTZERNAME/gems/bin/bundle exec /home/IHR_BENUTZERNAME/gems/bin/jekyll build -w'CPUQuota=20%[Install]WantedBy=multi-user.targetWichtig: Ersetzen Sie IHR_BENUTZERNAME durch Ihren tatsächlichen Benutzernamen und GITHUB_USERNAME durch Ihren GitHub-Benutzernamen. Hinweis: Die Zeile CPUQuota=20% sorgt dafür, dass der Dienst maximal 20% der verfügbaren CPU-Zeit verbraucht. Dies ist besonders sinnvoll, wenn Sie den Dienst auf einem Raspberry Pi betreiben, um die Systemressourcen zu schonen.Schritt 5: Dienst aktivieren und startenAktivieren und starten Sie den Dienst mit den folgenden Befehlen:sudo systemctl enable jekyll.servicesudo systemctl start jekyll.serviceFehlerbehebungFalls Sie auf Probleme stoßen, überprüfen Sie bitte: Sind alle Pfade korrekt? Hat der angegebene Benutzer die notwendigen Berechtigungen?Bei spezifischen Gem-Problemen kann es hilfreich sein, diese manuell zu installieren oder verschiedene Versionen zu testen. Nutzen Sie außerdem die Logs zur Fehlersuche:sudo journalctl -u jekyll.service -xeDienststatus prüfenNachdem der Dienst gestartet wurde, kann sein Status mit folgendem Befehl überprüft werden:systemctl status jekyll.serviceBeispielausgabe:● jekyll.service - Jekyll Build Loaded: loaded (/etc/systemd/system/jekyll.service; enabled; vendor preset: enabled) Active: active (running) since Mon 2025-03-17 08:04:20 UTC; 23h ago Main PID: 116 (bundle) Tasks: 8 (limit: 76807) Memory: 153.9M CPU: 55.466s CGroup: /system.slice/jekyll.service ├─116 \"/home/christian/gems/bin/jekyll build -w\" └─510 /home/christian/Madchristian.github.io/vendor/bundle/ruby/3.0.0/gems/sass-embedded-1.63.6/ext/sas&gt;Falls der Dienst unerwartet stoppt oder nicht startet, überprüfe die Logs mit:sudo journalctl -u jekyll.service -xeZusätzlich sollten etwaige Konflikte beim Generieren der Website behoben werden, wie sie in der Logausgabe erscheinen:Conflict: The following destination is shared by multiple files.The written file may end up with unexpected contents.Dies kann auftreten, wenn mehrere Dateien denselben Zielpfad haben. Stelle sicher, dass keine doppelten Kategorien oder Tags existieren.Falls weiterhin Probleme auftreten, könnte ein erneutes Bereinigen und Bauen der Site helfen:bundle exec jekyll cleanbundle exec jekyll buildDieser Abschnitt hilft Nutzern, typische Probleme mit dem systemd-Dienst zu erkennen und zu beheben.AbschlussJetzt sollte Ihre Jekyll-Instanz erfolgreich auf dem Server laufen und nach einem Systemneustart automatisch wieder gestartet werden. Überprüfen Sie den Status des Dienstes jederzeit mit:systemctl status jekyll.serviceMit diesen Schritten haben Sie eine robuste Konfiguration für Ihre Jekyll-Website unter systemd geschaffen." }, { "title": "mDNS mit VLAN nutzen (z.B. Raspberry Pi)", "url": "/posts/mDNS-refelector-with-vlan-avahi/", "categories": "Homelab", "tags": "pi, mdns, vlan, avahi, raspberry", "date": "2022-07-31 17:00:00 +0200", "snippet": "EinleitungIn diesem Beitrag erfahren Sie, wie Sie mDNS (Multicast DNS) in einem VLAN-Setup nutzen können, insbesondere mit einem Raspberry Pi. Diese Anleitung hilft Ihnen, Probleme wie den fehlende...", "content": "EinleitungIn diesem Beitrag erfahren Sie, wie Sie mDNS (Multicast DNS) in einem VLAN-Setup nutzen können, insbesondere mit einem Raspberry Pi. Diese Anleitung hilft Ihnen, Probleme wie den fehlenden Zugriff auf Drucker und Apple Airplay zu beheben, die häufig in Netzwerken mit VLANs auftreten.Probleme Apple Airprint funktioniert nicht. Zugriff auf Drucker und Apple Airplay ist nicht möglich. Omada SDN unterstützt derzeit keinen mDNS-Reflektor.SetupHardware-AnforderungenEin Raspberry Pi sollte im “Client”-Netzwerk mit getaggtem WLAN und IOT LAN VLAN (bei mir 70, 72 und 40) über eth0 verbunden sein.Installation des Avahi-DaemonsFühren Sie die folgenden Befehle aus, um den Avahi-Daemon zu installieren:sudo apt update &amp;&amp; sudo apt upgradesudo apt install avahi-daemonDaemon startenStarten Sie den Daemon mit einem der folgenden Befehle:/etc/init.d/avahi-daemon startoder:sudo systemctl start avahi-daemonUm den Daemon beim Booten automatisch zu starten, verwenden Sie:sudo systemctl enable avahi-daemonAvahi-Daemon konfigurierenÖffnen Sie die Konfigurationsdatei:sudo nano /etc/avahi/avahi-daemon.confFügen Sie die folgenden Einstellungen hinzu oder ändern Sie diese:[server]use-ipv4=yesuse-ipv6=yes[wide-area]enable-wide-area=yes[reflector]enable-reflector=yes#reflect-ipv=no#reflect-filters=_airplay._tcp.local,_raop._tcp.localVLAN im Network InterfaceErstellen Sie eine Konfigurationsdatei für die VLANs:sudo nano /etc/network/interfaces.d/vlansFügen Sie die folgenden VLAN-Konfigurationen hinzu:# WLANauto eth0.70iface eth0.70 inet manual vlan-raw-device eth0# IOT WLAN/LANauto eth0.72iface eth0.72 inet manual vlan-raw-device eth0# Client VLANauto eth0.40iface eth0.40 inet manual vlan-raw-device eth0AbschlussStarten Sie den Avahi-Daemon neu, um die Änderungen zu übernehmen:sudo systemctl restart avahi-daemonÜberprüfung der NetzwerkkonfigurationÜberprüfen Sie die Netzwerkkonfiguration mit dem folgenden Befehl:ip aSie sollten Ausgaben ähnlich der folgenden sehen:eth0.70@eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default qlen 1000 inet 10.0.70.155/24 brd 10.0.70.255 scope global dynamic noprefixroute eth0.70...FazitMit dieser Konfiguration sollten Airprint und Airplay über getrennte VLANs hinweg funktionieren." }, { "title": "Technologie, Automatisierung und Künstliche Intelligenz – Der Blog von Christian Strube", "url": "/posts/Willkommen/", "categories": "Technologie", "tags": "gpt-4,, künstliche, intelligenz,, technologie,, devops,, self-hosting,, macos,, monitoring", "date": "2022-01-01 01:00:00 +0100", "snippet": "Willkommen auf dem Blog von Christian Strube, einem Ort, an dem Sie sich auf eine faszinierende Reise durch die Welt der Technologie und der Künstlichen Intelligenz (KI) begeben können. Hier finden...", "content": "Willkommen auf dem Blog von Christian Strube, einem Ort, an dem Sie sich auf eine faszinierende Reise durch die Welt der Technologie und der Künstlichen Intelligenz (KI) begeben können. Hier finden Sie spannende Beiträge zu den Themen Technologie, Automatisierung und die neuesten Entwicklungen im Bereich der KI.Tauchen Sie ein in die Welt der neuesten Technologie-Trends, von aufstrebenden Technologien über Hardware bis hin zu Softwarelösungen. Erkunden Sie die Vielfalt der Möglichkeiten, die uns die fortschrittliche Technologie bietet, und entdecken Sie, wie sie unseren Alltag beeinflusst und verbessert.Ein weiterer Schwerpunkt dieses Blogs ist das spannende und ständig weiterentwickelnde Feld der Künstlichen Intelligenz (KI). Mit tiefergehenden Einblicken in die Mechanismen und Potenziale der KI können Sie verstehen, wie diese Technologie unsere Welt revolutioniert. Von der Verbesserung von Geschäftsprozessen über den Einsatz in der Medizin bis hin zu ihrem Einfluss auf unser tägliches Leben - die KI ist eine faszinierende Reise wert.Schauen Sie regelmäßig vorbei, um keine neuen Beiträge zu verpassen und sich kontinuierlich über spannende Themen zu informieren. Ich lade Sie ein, an der Diskussion teilzunehmen und Ihre Gedanken zu teilen. Willkommen in der Welt von Christian Strube - einer Welt voller technologischer Entdeckungen und Innovationen." } ]
